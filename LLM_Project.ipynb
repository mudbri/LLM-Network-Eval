{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LNK9GhRoZk9v"
      },
      "source": [
        "# <b>EDA of MCQ Data<b>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Basic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Imports:\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# Normal scores\n",
        "def printPercentages(header, num, denom):\n",
        "  print(f\"{header}: {num}/{denom} ({round(float(num)/denom*100,2)}%)\")\n",
        "\n",
        "def isFileinCategory(filename, category):\n",
        "    if category == \"all\":\n",
        "        return True\n",
        "    elif category == \"Grad\":\n",
        "        if \"grad\" in filename:\n",
        "            return True\n",
        "        if \"_\" in filename and filename.split('_')[0].isdigit():\n",
        "            return True\n",
        "    elif category == \"Basic\":\n",
        "        if \"google\" in filename:\n",
        "            return True\n",
        "        if \"Computer_Networking\" in filename:\n",
        "            return True\n",
        "    elif category == \"Cisco\":\n",
        "        if \"chapter\" in filename:\n",
        "            return True\n",
        "        elif \"NetSec\" in filename:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_total_score(folder_path, category, inference_level):\n",
        "    total_score_numerator = 0\n",
        "    total_score_denominator = 0\n",
        "\n",
        "    # Iterate through all CSV files in the specified folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Read the CSV file into a pandas DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Check if the \"Correct?\" column exists\n",
        "            if 'Correct?' in df.columns:\n",
        "                # Calculate the sum of correct values in the \"Correct?\" column\n",
        "                score_numerator = df['Correct?'].sum()\n",
        "\n",
        "                # Exclude the header when calculating the number of rows\n",
        "                score_denominator = len(df)\n",
        "\n",
        "                addition = 0\n",
        "                if inference_level == 1:\n",
        "                    addition = df[df['AQ - Inferrable(0-2)?'] == '2'].shape[0]\n",
        "                    addition += df[df['AQ - Inferrable(0-2)?'] == '1'].shape[0]\n",
        "                elif inference_level == 2:\n",
        "                    addition = df[df['AQ - Inferrable(0-2)?'] == '2'].shape[0]\n",
        "\n",
        "                # Print the score for each file as a fraction\n",
        "                # print(f\"{filename}: Score = {score_numerator}/{score_denominator}\")\n",
        "\n",
        "                # Add the scores to the total\n",
        "                total_score_numerator += score_numerator+addition\n",
        "                total_score_denominator += score_denominator\n",
        "            else:\n",
        "                print(f\"{filename}: 'Correct?' column not found\")\n",
        "\n",
        "    # Print the total score as a fraction\n",
        "    return total_score_numerator, total_score_denominator\n",
        "\n",
        "# Right minus wrong scores\n",
        "def calculate_num_choices(text):\n",
        "    choices = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        if len(line) > 1 and line[1] == \".\":\n",
        "            choices.append(line[0])\n",
        "    return choices\n",
        "\n",
        "def calculate_score_Right_minus_wrong(row, inference_level):\n",
        "    choices = calculate_num_choices(row[\"Choices\"])\n",
        "    num_choices = len(choices)\n",
        "\n",
        "    correct_answers = row[\"Correct Answer\"].strip().split(\",\")\n",
        "    if \"New Correct\" in row: # for self-checking\n",
        "        if str(row[\"New LLM Answer\"]) == \"nan\":\n",
        "            return 0\n",
        "        student_answers = row[\"New LLM Answer\"].strip().split(\",\")\n",
        "    else:\n",
        "        if int(row[\"Correct?\"]) == 1:\n",
        "            return 1\n",
        "        if str(row[\"LLM Answer\"]) == \"nan\":\n",
        "            print(row)\n",
        "        student_answers = row[\"LLM Answer\"].strip().split(\",\")\n",
        "        if inference_level != 0:\n",
        "            row_inferrable = int(row[\"AQ - Inferrable(0-2)?\"])\n",
        "            if row_inferrable != 0 and row_inferrable >= inference_level:\n",
        "                return 1\n",
        "\n",
        "    num_missing = 0\n",
        "    num_additional = 0\n",
        "    for answer in correct_answers:\n",
        "        if answer not in student_answers:\n",
        "            num_missing += 1\n",
        "\n",
        "    for answer in student_answers:\n",
        "        if answer.lower() not in correct_answers:\n",
        "            num_additional += 1\n",
        "\n",
        "    num_incorrect = num_additional+num_missing\n",
        "    num_correct = num_choices-num_incorrect\n",
        "\n",
        "    score = num_correct / num_choices - num_incorrect / num_choices\n",
        "\n",
        "    return max(0, score)  # Ensure score is non-negative\n",
        "\n",
        "def calculate_scores_for_file_Right_minus_wrong(csv_file, inference_level, detection):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    total_score = 0\n",
        "    total_questions = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if detection != 3:\n",
        "            if not pd.isnull(row[\"CD - detection student(1-3)\"]) and int(row[\"CD - detection student(1-3)\"]) != 3:\n",
        "                if detection == 2 and int(row[\"CD - detection student(1-3)\"]) <= 2:\n",
        "                    continue\n",
        "                elif detection == 1 and int(row[\"CD - detection student(1-3)\"]) == 1:\n",
        "                    continue\n",
        "        total_questions += 1\n",
        "        total_score += calculate_score_Right_minus_wrong(row, inference_level)\n",
        "    return total_score, total_questions\n",
        "\n",
        "\n",
        "# Right minus wrong - Users receive points equal to the number of right answers they choose minus the number of incorrect answers they choose. To determine how much each answer is worth, the system takes the total number of points assigned to the question and divides it by the total number of answer choices. For example, if a question is worth 10 points and has 5 answer choices, each correct answer is worth 2 points, and each incorrect answer is worth - 2 points (10/5 = 2). If a user gives 3 correct answers and 2 incorrect answers, 2 is the total number of points received for the question [(3*2)+(2*-2)]. Users can receive a minimum of zero on a question; they cannot receive a negative mark.\n",
        "def calculate_total_score_Right_minus_wrong(folder_path, category, inference_level, detection):\n",
        "    total_score = 0\n",
        "    total_ques = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            total_score_tmp, total_ques_tmp = calculate_scores_for_file_Right_minus_wrong(file_path, inference_level, detection)\n",
        "            total_score += total_score_tmp\n",
        "            total_ques += total_ques_tmp\n",
        "    # Print the total score as a fraction\n",
        "    return total_score, total_ques\n",
        "\n",
        "def printScoresAllLLMs(base, category, inference_level=0):\n",
        "    num, denom = calculate_total_score(base+\"/Claude3\", category, inference_level)\n",
        "    printPercentages(\"Total Score for Claude3\", num, denom)\n",
        "    num, denom = calculate_total_score(base+\"/GPT4\", category, inference_level)\n",
        "    printPercentages(\"Total Score for GPT4\", num, denom)\n",
        "    num, denom = calculate_total_score(base+\"/GPT3.5\", category, inference_level)\n",
        "    printPercentages(\"Total Score for GPT3.5\", num, denom)\n",
        "\n",
        "# inference level of 0 means cannot be infered. Inferenece level of 1 means can be infered by an expert. Inference level of 2 means answer is in the explanation and anyone can infer the correct answer\n",
        "# Detection of 3 means cannot be detected. Detection of 2 means can be detected with detailed reading. Detection of 1 means can be detected quickly\n",
        "def printRightMinusWrongScoresAllLLMs(base,category, inference_level=0, detection=3):\n",
        "    num, denom = calculate_total_score_Right_minus_wrong(base+\"/Claude3\", category, inference_level, detection)\n",
        "    printPercentages(\"Total right-minus-wrong score for Claude3\", num, denom)\n",
        "    num, denom = calculate_total_score_Right_minus_wrong(base+\"/GPT4\", category, inference_level, detection)\n",
        "    printPercentages(\"Total right-minus-wrong score for GPT4\", num, denom)\n",
        "    num, denom = calculate_total_score_Right_minus_wrong(base+\"/GPT3.5\", category, inference_level, detection)\n",
        "    printPercentages(\"Total right-minus-wrong score for GPT3.5\", num, denom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printRightMinusWrongScoresDifference(base1, base2, category, inference_level=0, detection=3):\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/Claude3\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/Claude3\", category, inference_level, detection)\n",
        "    print(\"Total right-minus-wrong score difference for Claude3\", ((num2/denom2)-(num1/denom1))*100, \"%\")\n",
        "\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/GPT4\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/GPT4\", category, inference_level, detection)\n",
        "    print(\"Total right-minus-wrong score difference for GPT4\", ((num2/denom2)-(num1/denom1))*100, \"%\")\n",
        "\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/GPT3.5\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/GPT3.5\", category, inference_level, detection)\n",
        "    print(\"Total right-minus-wrong score difference for GPT3.5\", ((num2/denom2)-(num1/denom1))*100, \"%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Misunderstanding types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot for misunderstanding reasons\n",
        "import csv\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "# settings for correcting Type3 font errors on HotCRP\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "# change default fonts and font families\n",
        "matplotlib.rcParams['font.sans-serif'] = \"Helvetica\"\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "\n",
        "def count_word_frequencies(folder_path, colm, category):\n",
        "    column_name = colm\n",
        "    word_frequencies = defaultdict(int)\n",
        "    \n",
        "    # Iterate over all CSV files in the given folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "                for row in reader:\n",
        "                    if column_name not in row:\n",
        "                        print(filename)\n",
        "                    if row[column_name]:\n",
        "                        # Split the string into words and count their frequencies\n",
        "                        words = row[column_name].replace(\", \",\",\").split(',')\n",
        "                        if \"Not having some information/concept\" in words:\n",
        "                            words.remove(\"Not having some information/concept\")\n",
        "                            if \"Having the wrong information/concept\" not in words:\n",
        "                                words.append(\"Having the wrong information/concept\")\n",
        "                        for word in words:\n",
        "                            word = word.strip()  # Remove leading/trailing whitespace\n",
        "                            if \"IP Range\" in word:\n",
        "                                word = \"Error in interpreting IPs\"\n",
        "                            if \"IP addresses\" in word:\n",
        "                                word = \"Error in interpreting IPs\"\n",
        "                            if \"interpreting ips\" in word:\n",
        "                                word = \"Error in interpreting IPs\"\n",
        "                            if word == \"Incorrect Choice\":\n",
        "                                word = \"Incorrect choice\"\n",
        "                            if word == \"Incorrect choice\":\n",
        "                                word = \"Selecting the wrong choice\"\n",
        "                            if word == \"Interpreted a word or background incorrectly\":\n",
        "                                word = \"Misinterpreting a word\"\n",
        "                            if word == \"Having the wrong information/concept\":\n",
        "                                word = \"Incorrect information/concept\"\n",
        "                            if word == \"Internal inconsistency\":\n",
        "                                word = \"Contradictory reasoning\"\n",
        "                            if word == \"Not being able to derive the correct conclusion\":\n",
        "                                word = \"Faulty inference\"\n",
        "                            if word == \"trusted article\":\n",
        "                                word = \"Article\"\n",
        "                            word = word.capitalize()\n",
        "                            if \"interpreting ips\" in word:\n",
        "                                word = \"Error in interpreting IPs\"\n",
        "                            if word == \"Rfc\":\n",
        "                                word = \"RFC\"\n",
        "                            word_frequencies[word] += 1\n",
        "\n",
        "    sorted_dict = dict(sorted(word_frequencies.items()))\n",
        "    return sorted_dict\n",
        "\n",
        "def plot_with_gnuplot(word_frequencies):\n",
        "    data_file_name = \"temp_data.dat\"\n",
        "    with open(data_file_name, 'w') as datafile:\n",
        "        for word, frequency in word_frequencies.items():\n",
        "            datafile.write(f'\"{word}\" {frequency}\\n')\n",
        "    \n",
        "    gnuplot_script = \"\"\"\n",
        "    set terminal png size 800,600\n",
        "    set output 'reasons_type_Claude3_.png'\n",
        "    set boxwidth 0.5\n",
        "    set offset 0, 0, 0.1, 0\n",
        "    set yrange [0:*]\n",
        "    set style fill solid\n",
        "    set ylabel 'Frequency'\n",
        "    set xlabel 'Words'\n",
        "    set xtics rotate by -45\n",
        "    plot 'temp_data.dat' using 2:xtic(1) with boxes notitle\n",
        "    \"\"\"\n",
        "    \n",
        "    subprocess.run(['gnuplot'], input=gnuplot_script, text=True)\n",
        "    \n",
        "    # Cleanup\n",
        "    try:\n",
        "        subprocess.run(['rm', data_file_name])\n",
        "    except Exception as e:\n",
        "        print(f\"Error removing temporary data file: {e}\")\n",
        "\n",
        "def plot_with_matplotlib(word_frequencies, k=4):\n",
        "    # Sort the word frequencies dictionary by values in descending order and select the top k entries\n",
        "    sorted_word_frequencies = dict(sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)[:k])\n",
        "\n",
        "    # Extracting words and their frequencies\n",
        "    words = list(sorted_word_frequencies.keys())\n",
        "    frequencies = list(sorted_word_frequencies.values())\n",
        "\n",
        "    # Plotting word frequencies\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.bar(words, frequencies, color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Word Frequencies')\n",
        "    plt.xticks(rotation=90)  # Rotating x-axis labels for better readability\n",
        "    plt.show()\n",
        "\n",
        "# Function to get the top k words and their frequencies from a given dictionary\n",
        "def get_top_k_words(word_freq, k):\n",
        "    sorted_word_freq = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True)[:k])\n",
        "    return sorted_word_freq\n",
        "\n",
        "def plot_with_matplotlib_all_three_reasons(claude3, gpt4, gpt35, k=4, ylim=53, filename=\"\", rotation=15):\n",
        "    # Get top k words for each set of word frequencies\n",
        "    top_k_words_1 = get_top_k_words(claude3, k)\n",
        "    top_k_words_2 = get_top_k_words(gpt4, k)\n",
        "    top_k_words_3 = get_top_k_words(gpt35, k)\n",
        "\n",
        "    # Extracting words and their frequencies for each set of top k word frequencies\n",
        "    words_1 = list(top_k_words_1.keys())\n",
        "    frequencies_1 = list(top_k_words_1.values())\n",
        "\n",
        "    words_2 = list(top_k_words_2.keys())\n",
        "    frequencies_2 = list(top_k_words_2.values())\n",
        "\n",
        "    words_3 = list(top_k_words_3.keys())\n",
        "    frequencies_3 = list(top_k_words_3.values())\n",
        "\n",
        "    # Plotting top k word frequencies side by side\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "    # Plot for top k word frequencies 1\n",
        "    axs[0].bar(words_1, frequencies_1, color='skyblue')\n",
        "    # axs[0].set_xlabel('Reasons')\n",
        "    axs[0].set_ylabel('Frequencies')\n",
        "    axs[0].set_title('Claude 3')\n",
        "    axs[0].tick_params(axis='x', rotation=rotation)\n",
        "\n",
        "    # Plot for top k word frequencies 2\n",
        "    axs[1].bar(words_2, frequencies_2, color='lightgreen')\n",
        "    # axs[1].set_xlabel('Reasons')\n",
        "    axs[1].set_ylabel('Frequencies')\n",
        "    axs[1].set_title('GPT-4')\n",
        "    axs[1].tick_params(axis='x', rotation=rotation)\n",
        "\n",
        "    # Plot for top k word frequencies 3\n",
        "    axs[2].bar(words_3, frequencies_3, color='salmon')\n",
        "    # axs[2].set_xlabel('Reasons')\n",
        "    axs[2].set_ylabel('Frequencies')\n",
        "    axs[2].set_title('GPT-3.5')\n",
        "    axs[2].tick_params(axis='x', rotation=rotation)\n",
        "    axs[0].set_ylim([0, ylim]) \n",
        "    axs[1].set_ylim([0, ylim]) \n",
        "    axs[2].set_ylim([0, ylim]) \n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    if len(filename) > 1:\n",
        "        fig.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "\n",
        "def plot_all_llms_reasons(base, category=\"all\", k=4, ylim=53, filename=\"\"):\n",
        "    word_frequencies_claude = count_word_frequencies(base+\"/Claude3/\", colm=\"SM - Misunderstanding Reasons\", category=category)\n",
        "    word_frequencies_gpt4 = count_word_frequencies(base+\"/GPT4/\", colm=\"SM - Misunderstanding Reasons\", category=category)\n",
        "    word_frequencies_gpt35 = count_word_frequencies(base+\"/GPT3.5/\", colm=\"SM - Misunderstanding Reasons\", category=category)\n",
        "    plot_with_matplotlib_all_three_reasons(word_frequencies_claude, word_frequencies_gpt4, word_frequencies_gpt35, k, ylim, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot for misunderstanding types\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "def plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentage (%)', rotation=45, legend_loc=0, min=50, labels=['Claude 3','GPT-4','GPT-3.5'], bar_width = 0.25, figsize=(6,2), filename=\"performance.pdf\", x_label_on=True):\n",
        "    # Extracting words and their frequencies for each set of word frequencies\n",
        "    words = list(gpt35.keys())\n",
        "\n",
        "    # Set the width of the bars\n",
        "    bar_width = bar_width\n",
        "\n",
        "    # Set the position of the bars on the x-axis\n",
        "    r1 = np.arange(len(words))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    # Plotting word frequencies side by side\n",
        "    f = plt.figure(figsize=figsize)\n",
        "\n",
        "    plt.bar(r1, list(claude3.values()), color='skyblue', width=bar_width, edgecolor='grey', label=labels[0], hatch= '///')\n",
        "    plt.bar(r2, list(gpt4.values()), color='lightgreen', width=bar_width, edgecolor='grey', label=labels[1],  hatch= '...')\n",
        "    plt.bar(r3, list(gpt35.values()), color='salmon', width=bar_width, edgecolor='grey', label=labels[2])\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    if x_label_on:\n",
        "        plt.xlabel(xlabel, fontweight='bold')\n",
        "    plt.ylabel(ylabel, fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(words))], words, rotation=rotation)\n",
        "    plt.ylim(min, 101) \n",
        "\n",
        "    # Create legend & Show graphic\n",
        "    if legend_loc == 0:\n",
        "        plt.legend(loc=\"best\")\n",
        "    else:\n",
        "        plt.legend(loc=legend_loc)\n",
        "    # plt.title('Type of Misunderstandings')\n",
        "    plt.show()\n",
        "    if len(filename) > 1:\n",
        "        f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n",
        "def count_category_frequencies(folder_path, category_filename):\n",
        "    # Mapping of old categories to new, general categories\n",
        "    category_mapping = {\n",
        "        \"Wrong Information/Hallucinations\": \"Incorrect information/concept\",\n",
        "        # \"Corner Case\": \"Wrong Facts/Concepts\",\n",
        "        \"Conceptual Misunderstanding\": \"Incorrect information/concept\",\n",
        "        \"Misunderstanding context or implied background\": \"Misinterpreting question\",\n",
        "        \"Otherwise didn't understand the goal of the question\": \"Misinterpreting question\",\n",
        "        \"Misinterpreting questions\": \"Misinterpreting question\",\n",
        "        # \"Quantifier (universal vs existential)\": \"Misinterpreting questions\",\n",
        "        # \"Direct vs Indirect Causation\": \"Misinterpreting questions\",\n",
        "        # \"Didn't understand a word or format\": \"Misinterpreting questions\",\n",
        "        # \"Didn't understand logical syntax\": \"Misinterpreting questions\",\n",
        "        # \"Incorrect copying\": \"Misinterpreting questions\",\n",
        "        \"Incorrect reasoning/deduction\": \"Reasoning Error\",\n",
        "        \"Wrong Facts/Concept\": \"Incorrect information/concept\",\n",
        "        # \"Incorrect maths\": \"Reasoning Error\"\n",
        "    }\n",
        "\n",
        "    general_category_frequencies = defaultdict(int)\n",
        "    \n",
        "    # Iterate over all CSV files in the given folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category_filename):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "                for row in reader:\n",
        "                    if row[\"SM - Misunderstanding General\"]:\n",
        "                        # Split the string into categories and map them to general categories\n",
        "                        categories = row[\"SM - Misunderstanding General\"].split(',')\n",
        "                        current_categories = []\n",
        "                        for category in categories:\n",
        "                            category = category.strip()  # Remove leading/trailing whitespace\n",
        "                            if category in category_mapping:\n",
        "                                general_category = category_mapping[category]\n",
        "                                if general_category not in current_categories:\n",
        "                                    current_categories.append(general_category)\n",
        "                                    general_category_frequencies[general_category] += 1\n",
        "                            else:\n",
        "                                print(\"Category not found:\", category)\n",
        "    \n",
        "    total_errors = general_category_frequencies[\"Incorrect information/concept\"]+general_category_frequencies[\"Misinterpreting question\"]+general_category_frequencies[\"Reasoning Error\"]\n",
        "    newDict = {} # this is done to fix ordering\n",
        "    newDict[\"Incorrect information/concept\"] = general_category_frequencies[\"Incorrect information/concept\"]*100/total_errors\n",
        "    newDict[\"Misinterpreting question\"] = general_category_frequencies[\"Misinterpreting question\"]*100/total_errors\n",
        "    newDict[\"Reasoning Error\"] = general_category_frequencies[\"Reasoning Error\"]*100/total_errors\n",
        "    \n",
        "    return newDict\n",
        "\n",
        "def plot_all_llms_types(base, category=\"all\", filename=\"\"):\n",
        "    word_frequencies_claude = count_category_frequencies(base+\"/Claude3/\", category_filename=category)\n",
        "    word_frequencies_gpt4 = count_category_frequencies(base+\"/GPT4/\", category_filename=category)\n",
        "    word_frequencies_gpt35 = count_category_frequencies(base+\"/GPT3.5/\", category_filename=category)\n",
        "    plot_with_matplotlib_all_three_types(word_frequencies_claude, word_frequencies_gpt4, word_frequencies_gpt35, 4, min=0, rotation=10, filename=filename, x_label_on=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Misconceptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Misconceptions\n",
        "def count_misconceptions(folder1, category):\n",
        "    count = 0\n",
        "    questions_misconceptions0 = []\n",
        "    questions_misconceptions1 = []\n",
        "    \n",
        "    # Process files in folder1\n",
        "    for filename in os.listdir(folder1):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category):\n",
        "            filepath = os.path.join(folder1, filename)\n",
        "            with open(filepath, 'r') as file:\n",
        "                reader = csv.DictReader(file)\n",
        "                for row in reader:\n",
        "                    if row['Effect - Conceptual error in explanaiton?(0/1)'] == '':\n",
        "                        continue\n",
        "                    if float(row['Effect - Conceptual error in explanaiton?(0/1)']) == 0:\n",
        "                        questions_misconceptions0.append(row['Question Number'])\n",
        "                    if float(row['Effect - Conceptual error in explanaiton?(0/1)']) == 1:\n",
        "                        questions_misconceptions1.append(row['Question Number'])\n",
        "    \n",
        "    num_misconceptions = len(questions_misconceptions1)\n",
        "    num_incorrect = num_misconceptions+len(questions_misconceptions0)\n",
        "    return num_misconceptions, num_incorrect\n",
        "\n",
        "def printMisconceptions(base, category):\n",
        "    num, denom = count_misconceptions(base+\"/Claude3\", category)\n",
        "    printPercentages(\"Total misconceptions for Claude3\", num, denom)\n",
        "    num, denom = count_misconceptions(base+\"/GPT4\", category)\n",
        "    printPercentages(\"Total misconceptions for GPT4\", num, denom)\n",
        "    num, denom = count_misconceptions(base+\"/GPT3.5\", category)\n",
        "    printPercentages(\"Total misconceptions for GPT3.5\", num, denom)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Effects Subtopics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot for Effects\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "effect_subtopics = {\n",
        "    # \"Configurations\": [\"Configuring EtherChannel\", \"WLAN Configuration\", \"Configuring switches\", \"Administrative Distances\"],\n",
        "    \"Basic networking\": [\"IP Translation\", \"IP Ranges\", \"IP Subnetting\", \"IP Class Ranges\", \"Layer 3 Switches\", \"IP Routing\", \"CIDR\", \"DNS Queries\", \"Packet Switching\", \"CDNs\", \"TCP\", \"Network Architecture\", \"Internet protocol suite\", \"OSPF\", \"IPv4\", \"ARP\", \"Routers\", \"BGP\", \"NAT/PAT\", \"Virtual Networks\", \"BOOTP\", \"Server\", \"POTS\", \"terminology\", \"IPv6\", \"Wireless\"],\n",
        "\n",
        "    \"Network security\": [\"ACL mask\", \"ACL numbers\", \"ACL rules processing\", \"CSMA\", \"DDoS\", \"Spoofing\", \"Two-factor Authentication\", \"Proxies\", \"Network Attacks\", \"WAN security\", \"ACL capabilities\"],\n",
        "\n",
        "    \"Network administration\": [\"CISCO Commands\", \"Cisco DNA Center\", \"mininet\", \"Mininet\", \"SSH\", \"Network utilities\", \"Ethernet Cables\", \"QoS characteristics\", \"Router Hardware\", \"layer-2 MAC topology\", \"Configuring EtherChannel\", \"WLAN Configuration\", \"Configuring switches\", \"Administrative Distances\", \"E-Lan Configuration\", \"Logging\", \"IEEE 802.11 Wi-Fi standards\"],\n",
        "\n",
        "    \"Advanced networking\": [\"Protocol Independent Forwarding\", \"P4\", \"Click\", \"Ethernet GRE\", \"OpenFlow Rule Composition\", \"SDNs\", \"4D Network Architecture\", \"OpenFlow Code\", \"Nicira\", \"FlowVisor\", \"Flowspaces\", \"Pyretic\", \"NFV\", \"SwitchBlade\", \"NetASM\", \"NVP\", \"PortLand\",\"Configuration Verification\",\"Data Plane Verification\", \"OpenFlow\", \"Routing Control Platform\", \"SDX\", \"SDN in Home Networks\", \"SDN Controllers\"]\n",
        "}\n",
        "\n",
        "def reverse_dictionary(dictionary):\n",
        "    new_dictionary = {}\n",
        "    for key, values in dictionary.items():\n",
        "        for value in values:\n",
        "            new_dictionary[value] = key\n",
        "    return new_dictionary\n",
        "\n",
        "def plot_with_matplotlib_all_four_subtopics(claude3, gpt4, gpt35, filename):\n",
        "    # Extracting words and their frequencies for each set of word frequencies\n",
        "    words = list(effect_subtopics.keys())\n",
        "\n",
        "    # Set the width of the bars\n",
        "    bar_width = 0.25\n",
        "\n",
        "    # Set the position of the bars on the x-axis\n",
        "    r1 = np.arange(len(words))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    # Plotting word frequencies side by side\n",
        "    f = plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.bar(r1, list(claude3.values()), color='skyblue', width=bar_width, edgecolor='grey', label='Claude 3', hatch=\"///\")\n",
        "    plt.bar(r2, list(gpt4.values()), color='lightgreen', width=bar_width, edgecolor='grey', label='GPT-4', hatch=\"...\")\n",
        "    plt.bar(r3, list(gpt35.values()), color='salmon', width=bar_width, edgecolor='grey', label='GPT-3.5')\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.xlabel('Concepts', fontweight='bold')\n",
        "    plt.ylabel('Frequencies', fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(words))], words, rotation=0)\n",
        "\n",
        "    # Create legend & Show graphic\n",
        "    plt.legend()\n",
        "    # plt.title('Type of Misunderstandings')\n",
        "    plt.show()\n",
        "    if len(filename) > 1:\n",
        "        f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "def count_category_frequencies_subtopics(folder_path, category_filename, misconceptionsCheck=False):\n",
        "    # Mapping of old categories to new, general categories\n",
        "    general_category_frequencies = defaultdict(int)\n",
        "    category_mapping = reverse_dictionary(effect_subtopics)\n",
        "    # Iterate over all CSV files in the given folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category_filename):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "                for row in reader:\n",
        "                    if row[\"Effect - Subtopics\"]:\n",
        "                        if misconceptionsCheck == True and float(row['Effect - Conceptual error in explanaiton?(0/1)']) == 0: # ignore subtopics if they do not cause misconceptions\n",
        "                            continue\n",
        "                        # Split the string into categories and map them to general categories\n",
        "                        categories = row[\"Effect - Subtopics\"].split(',')\n",
        "                        current_categories = []\n",
        "                        for category in categories:\n",
        "                            category = category.strip()  # Remove leading/trailing whitespace\n",
        "                            if category in category_mapping:\n",
        "                                general_category = category_mapping[category]\n",
        "                                if general_category not in current_categories:\n",
        "                                    current_categories.append(general_category)\n",
        "                                    general_category_frequencies[general_category] += 1\n",
        "                            else:\n",
        "                                print(\"Category not in list:\", category)\n",
        "    \n",
        "    \n",
        "    total_errors = general_category_frequencies[\"Basic networking\"]+general_category_frequencies[\"Network security\"]+general_category_frequencies[\"Network administration\"]+general_category_frequencies[\"Advanced networking\"]\n",
        "    newDict = {} # this is done to fix ordering\n",
        "    newDict[\"Basic networking\"] = general_category_frequencies[\"Basic networking\"]*100/total_errors\n",
        "    newDict[\"Network security\"] = general_category_frequencies[\"Network security\"]*100/total_errors\n",
        "    newDict[\"Network administration\"] = general_category_frequencies[\"Network administration\"]*100/total_errors\n",
        "    newDict[\"Advanced networking\"] = general_category_frequencies[\"Advanced networking\"]*100/total_errors\n",
        "    # print(total_errors)\n",
        "    # print(general_category_frequencies[\"Basic Networking\"])\n",
        "    # print(general_category_frequencies[\"Network Security\"])\n",
        "    # print(general_category_frequencies[\"Concepts for Network Operators\"])\n",
        "    # print(general_category_frequencies[\"Advanced\"])\n",
        "    # print()\n",
        "    \n",
        "    # return newDict\n",
        "    return general_category_frequencies # Returning frequencies instead of percentage\n",
        "\n",
        "def plot_all_llms_subtopics(base, category=\"all\", misconceptionsCheck=False, filename=\"\"):\n",
        "    word_frequencies_claude = count_category_frequencies_subtopics(base+\"/Claude3/\", category_filename=category, misconceptionsCheck=misconceptionsCheck)\n",
        "    word_frequencies_gpt4 = count_category_frequencies_subtopics(base+\"/GPT4/\", category_filename=category, misconceptionsCheck=misconceptionsCheck)\n",
        "    word_frequencies_gpt35 = count_category_frequencies_subtopics(base+\"/GPT3.5/\", category_filename=category, misconceptionsCheck=misconceptionsCheck)\n",
        "    plot_with_matplotlib_all_four_subtopics(word_frequencies_claude, word_frequencies_gpt4, word_frequencies_gpt35, filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Improvement Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def equal_answers(row):\n",
        "    answer1 = str(row[\"LLM Answer_LLM1\"]).strip()\n",
        "    answer2 = str(row[\"LLM Answer_LLM2\"]).strip()\n",
        "    # if \" \" in answer1 or \" \" in answer2:\n",
        "    #     print(\"Potential error in format of answers\", answer1, answer2)\n",
        "    if str(answer1.strip()) == str(answer2.strip()): # all answers are already in alphabetical order\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "def calculate_score_Right_minus_wrong_raw(choices, student_answers, correct_answers):\n",
        "    choices = calculate_num_choices(choices)\n",
        "    student_answers = student_answers.strip().split(\",\")\n",
        "    correct_answers = correct_answers.strip().split(\",\")\n",
        "    num_choices = len(choices)\n",
        "    num_missing = 0\n",
        "    num_additional = 0\n",
        "    for answer in correct_answers:\n",
        "        if answer not in student_answers:\n",
        "            num_missing += 1\n",
        "\n",
        "    for answer in student_answers:\n",
        "        if answer.lower() not in correct_answers:\n",
        "            num_additional += 1\n",
        "\n",
        "    num_incorrect = num_additional+num_missing\n",
        "    num_correct = num_choices-num_incorrect\n",
        "\n",
        "    score = num_correct / num_choices - num_incorrect / num_choices\n",
        "\n",
        "    return max(0, score)  # Ensure score is non-negative\n",
        "\n",
        "def calculate_best_out_of_3_score(base, category):\n",
        "    GPT4 = read_LLM_data(base+\"/GPT4/\", category)\n",
        "    Claude3 = read_LLM_data(base+\"/Claude3/\", category)\n",
        "    GPT35 = read_LLM_data(base+\"/GPT3.5/\", category)\n",
        "\n",
        "    GPT4['Question ID'] = GPT4['Question Number'].astype(str) + '_' + GPT4['Topic']\n",
        "    Claude3['Question ID'] = Claude3['Question Number'].astype(str) + '_' + Claude3['Topic']\n",
        "    GPT35['Question ID'] = GPT35['Question Number'].astype(str) + '_' + GPT35['Topic']\n",
        "\n",
        "    GPT4.dropna(how='all', axis=1, inplace=True) \n",
        "    Claude3.dropna(how='all', axis=1, inplace=True) \n",
        "    GPT35.dropna(how='all', axis=1, inplace=True) \n",
        "\n",
        "    # # Merge dataframes on the unique identifier\n",
        "    merged_data_GPT4_GPT35 = pd.merge(GPT4, GPT35, on='Question ID', suffixes=('_LLM1', '_LLM2'))\n",
        "    merged_data_GPT4_Claude3 = pd.merge(GPT4, Claude3, on='Question ID', suffixes=('_LLM1', '_LLM2'))\n",
        "    merged_data_Claude3_GPT35 = pd.merge(Claude3, GPT35, on='Question ID', suffixes=('_LLM1', '_LLM2'))\n",
        "\n",
        "\n",
        "    total_score = 0\n",
        "    total_questions = 0\n",
        "    for index, row in merged_data_GPT4_GPT35.iterrows():\n",
        "        total_questions += 1\n",
        "        if equal_answers(row):\n",
        "            score = calculate_score_Right_minus_wrong_raw(choices=row[\"Choices_LLM1\"], student_answers=row[\"LLM Answer_LLM1\"], correct_answers=row[\"Correct Answer_LLM1\"])\n",
        "            total_score += score\n",
        "            continue\n",
        "        new_row = merged_data_GPT4_Claude3[merged_data_GPT4_Claude3['Question ID'] == row['Question ID']].squeeze()\n",
        "        if equal_answers(new_row):\n",
        "            score = calculate_score_Right_minus_wrong_raw(choices=new_row[\"Choices_LLM1\"], student_answers=new_row[\"LLM Answer_LLM1\"], correct_answers=new_row[\"Correct Answer_LLM1\"])\n",
        "            total_score += score\n",
        "            continue\n",
        "        new_row2 = merged_data_Claude3_GPT35[merged_data_Claude3_GPT35['Question ID'] == row['Question ID']].squeeze()\n",
        "        if equal_answers(new_row2):\n",
        "            score = calculate_score_Right_minus_wrong_raw(choices=new_row2[\"Choices_LLM1\"], student_answers=new_row2[\"LLM Answer_LLM1\"], correct_answers=new_row2[\"Correct Answer_LLM1\"])\n",
        "            total_score += score\n",
        "            continue\n",
        "        # If none of the LLMs match in answers, we consider Claude3's answers\n",
        "        new_row3 = Claude3[Claude3['Question ID'] == row['Question ID']].squeeze()\n",
        "        score = calculate_score_Right_minus_wrong_raw(choices=new_row3[\"Choices\"], student_answers=new_row3[\"LLM Answer\"], correct_answers=new_row3[\"Correct Answer\"])\n",
        "        total_score += score\n",
        "        # printPercentages(\"Total Score for combined LLMs:\",total_score,total_questions)\n",
        "    return total_score, total_questions\n",
        "    \n",
        "def printRightMinusWrongScoresDifference(base1, base2, category, inference_level=0, detection=3):\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/Claude3\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/Claude3\", category, inference_level, detection)\n",
        "    if denom1 != denom2:\n",
        "        print(\"Total number of questions are difference for {} and {} for Claude3\".format(base1, base2))\n",
        "    printPercentages(\"Total right-minus-wrong score difference for Claude3\", num2-num1, denom1)\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/GPT4\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/GPT4\", category, inference_level, detection)\n",
        "    if denom1 != denom2:\n",
        "        print(\"Total number of questions are difference for {} and {} for GPT4\".format(base1, base2))\n",
        "    printPercentages(\"Total right-minus-wrong score difference for GPT4\", num2-num1, denom1)\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base1+\"/GPT3.5\", category, inference_level, detection)\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base2+\"/GPT3.5\", category, inference_level, detection)\n",
        "    if denom1 != denom2:\n",
        "        print(\"Total number of questions are difference for {} and {} for GPT3.5\".format(base1, base2))\n",
        "    printPercentages(\"Total right-minus-wrong score difference for GPT3.5\", num2-num1, denom1)\n",
        "\n",
        "\n",
        "def printScoreDifferenceFromMajorityVoting(base, category, inference_level=0, detection=3):\n",
        "    num, denom = calculate_best_out_of_3_score(base, category)\n",
        "    num3, denom3 = calculate_total_score_Right_minus_wrong(base+\"/GPT3.5\", category, inference_level, detection)\n",
        "    if denom3 != denom:\n",
        "        print(\"Total number of questions are difference for for GPT3.5\")\n",
        "    printPercentages(\"Total right-minus-wrong score difference for GPT3.5\", num-num3, denom3)\n",
        "\n",
        "    num2, denom2 = calculate_total_score_Right_minus_wrong(base+\"/GPT4\", category, inference_level, detection)\n",
        "    if denom2 != denom:\n",
        "        print(\"Total number of questions are difference for for GPT4\")\n",
        "    printPercentages(\"Total right-minus-wrong score difference for GPT4\", num-num2, denom2)\n",
        "\n",
        "    num1, denom1 = calculate_total_score_Right_minus_wrong(base+\"/Claude3\", category, inference_level, detection)\n",
        "    if denom1 != denom:\n",
        "        print(\"Total number of questions are difference for for Claude3\")\n",
        "    printPercentages(\"Total right-minus-wrong score difference for Claude3\", num-num1, denom1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# For correlations\n",
        "from tabulate import tabulate\n",
        "\n",
        "def read_LLM_data(folder_path, category):\n",
        "    student_data = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".csv\") and isFileinCategory(file, category):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            df = pd.read_csv(file_path)\n",
        "            student_data.append(df)\n",
        "    return pd.concat(student_data, ignore_index=True)\n",
        "\n",
        "def compute_correlation(folder1_path, folder2_path, category):\n",
        "    LLM1_data = read_LLM_data(folder1_path, category)\n",
        "    LLM2_data = read_LLM_data(folder2_path, category)\n",
        "\n",
        "    # Combine \"Question Number\" and \"Topic\" to create a unique identifier\n",
        "    LLM1_data['Question ID'] = LLM1_data['Question Number'].astype(str) + '_' + LLM1_data['Topic']\n",
        "    LLM2_data['Question ID'] = LLM2_data['Question Number'].astype(str) + '_' + LLM2_data['Topic']\n",
        "    # Merge dataframes on the unique identifier\n",
        "    merged_data = pd.merge(LLM1_data, LLM2_data, on='Question ID', suffixes=('_LLM1', '_LLM2'))\n",
        "    # pd.options.display.max_columns = None\n",
        "    # pd.options.display.max_rows = None\n",
        "    # print(merged_data['Score-right-minus-wrong_LLM1'])  # Python 3.x\n",
        "    # print(merged_data['Score-right-minus-wrong_LLM2'])  # Python 3.x\n",
        "\n",
        "    # Compute correlation coefficient between the scores\n",
        "    correlation_coefficient = merged_data['Score-right-minus-wrong_LLM1'].corr(merged_data['Score-right-minus-wrong_LLM2'])\n",
        "\n",
        "    return correlation_coefficient\n",
        "\n",
        "def contingency_table(folder1_path, folder2_path, category):\n",
        "    LLM1_data = read_LLM_data(folder1_path, category)\n",
        "    LLM2_data = read_LLM_data(folder2_path, category)\n",
        "\n",
        "    # Combine \"Question Number\" and \"Topic\" to create a unique identifier\n",
        "    LLM1_data['Question ID'] = LLM1_data['Question Number'].astype(str) + '_' + LLM1_data['Topic']\n",
        "    LLM2_data['Question ID'] = LLM2_data['Question Number'].astype(str) + '_' + LLM2_data['Topic']\n",
        "\n",
        "    # Merge dataframes on the unique identifier\n",
        "    merged_data = pd.merge(LLM1_data, LLM2_data, on='Question ID', suffixes=('_LLM1', '_LLM2'))\n",
        "\n",
        "    LLM1_correct = merged_data[(merged_data['Score-right-minus-wrong_LLM1'] >= 1) & (merged_data['Score-right-minus-wrong_LLM2'] < 1)].shape[0]\n",
        "    LLM2_correct = merged_data[(merged_data['Score-right-minus-wrong_LLM2'] >= 1) & (merged_data['Score-right-minus-wrong_LLM1'] < 1)].shape[0]\n",
        "    both_correct = merged_data[(merged_data['Score-right-minus-wrong_LLM1'] >= 1) & (merged_data['Score-right-minus-wrong_LLM2'] >= 1)].shape[0]\n",
        "    both_incorrect = merged_data[(merged_data['Score-right-minus-wrong_LLM1'] < 1) & (merged_data['Score-right-minus-wrong_LLM2'] < 1)].shape[0]\n",
        "\n",
        "    return LLM1_correct, LLM2_correct, both_correct, both_incorrect\n",
        "\n",
        "def print_all_contingency_tables_normal_case(base, category):\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/Claude3/\", base+\"/GPT3.5/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"Claude3 Correct\",\"Claude3 Incorrect\"]\n",
        "    index = [\"GPT3.5 Correct\",\"GPT3.5 Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "    print()\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/Claude3/\", base+\"/GPT4/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"Claude3 Correct\",\"Claude3 Incorrect\"]\n",
        "    index = [\"GPT4 Correct\",\"GPT4 Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "    print()\n",
        "\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/GPT3.5/\", base+\"/GPT4/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"GPT3.5 Correct\",\"GPT3.5 Incorrect\"]\n",
        "    index = [\"GPT4 Correct\",\"GPT4 Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "\n",
        "def print_all_contingency_tables_reordered_case(base, base2, category):\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/Claude3/\", base2+\"/Claude3/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"Claude3 Correct\",\"Claude3 Incorrect\"]\n",
        "    index = [\"Claude3 Reordered Correct\",\"Claude3 Reordered Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "    print()\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/GPT4/\", base2+\"/GPT4/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"GPT4 Correct\",\"GPT4 Incorrect\"]\n",
        "    index = [\"GPT4 Reordered Correct\",\"GPT4 Reordered Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "    print()\n",
        "\n",
        "    LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(base+\"/GPT3.5/\", base2+\"/GPT3.5/\", category)\n",
        "    matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "    headers = [\"GPT3.5 Correct\",\"GPT3.5 Incorrect\"]\n",
        "    index = [\"GPT3.5 Reordered Correct\",\"GPT3.5 Reordered Incorrect\"]\n",
        "    num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "    diff = LLM1_correct+LLM2_correct\n",
        "    print(tabulate(matrix, headers=headers, showindex=index))\n",
        "    print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))\n",
        "\n",
        "# reference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/#:~:text=The%20strength%20of%20relationship%20can,also%20tends%20to%20do%20so).\n",
        "def correlation_rule_of_thumbs(correlation):\n",
        "    if (correlation > 0.9 and correlation <= 1) or (correlation < -0.9 and correlation >= -1):\n",
        "        return \"Very high correlation\"\n",
        "    elif (correlation > 0.7 and correlation <= 0.9) or (correlation < -0.7 and correlation >= -0.9):\n",
        "        return \"High correlation\"\n",
        "    elif (correlation > 0.5 and correlation <= 0.7) or (correlation < -0.5 and correlation >= -0.7):\n",
        "        return \"Moderate correlation\"\n",
        "    elif (correlation > 0.3 and correlation <= 0.5) or (correlation < -0.3 and correlation >= -0.5):\n",
        "        return \"Low correlation\"\n",
        "    else:\n",
        "        return \"Negligible correlation\"\n",
        "    \n",
        "\n",
        "def print_all_correlations_normal_case(base, category):\n",
        "    correlation_coefficient = compute_correlation(base+\"/Claude3/\", base+\"/GPT3.5/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between Claude3 and GPT3.5: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))\n",
        "\n",
        "    correlation_coefficient = compute_correlation(base+\"/Claude3/\", base+\"/GPT4/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between Claude3 and GPT4: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))\n",
        "\n",
        "    correlation_coefficient = compute_correlation(base+\"/GPT3.5/\", base+\"/GPT4/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between GPT3.5 and GPT4: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))\n",
        "\n",
        "def print_all_correlations_reordered_case(base, base2, category):\n",
        "    correlation_coefficient = compute_correlation(base+\"/Claude3/\", base2+\"/Claude3/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between Claude3 and Claude3 with reordered choices: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))\n",
        "\n",
        "    correlation_coefficient = compute_correlation(base+\"/GPT4/\", base2+\"/GPT4/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between GPT4 and GPT4 with reordered choices: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))\n",
        "\n",
        "    correlation_coefficient = compute_correlation(base+\"/GPT3.5/\", base2+\"/GPT3.5/\", category)\n",
        "    print(\"Correlation Coefficient between the scores between GPT3.5 and GPT3.5 with reordered choices: {} ({})\".format(correlation_coefficient, correlation_rule_of_thumbs(correlation_coefficient)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logprob Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logprob stats\n",
        "def average_logprob_general(folder_path, colm, keywords, category):\n",
        "    # Initialize total sum and count for averaging\n",
        "    total_sum = 0\n",
        "    count = 0\n",
        "    \n",
        "    # Iterate through each file in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\") and isFileinCategory(filename, category):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Read the CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "            # Iterate through each word and filter rows\n",
        "            if (\"0\" in keywords):\n",
        "                filtered_df = df[df[colm] == 0]\n",
        "                # Calculate the sum of the 'logprob Confidence' column for these rows\n",
        "                sum_logprob = filtered_df['logprob Confidence'].sum()\n",
        "                # Update the total sum and count\n",
        "                total_sum += sum_logprob\n",
        "                count += len(filtered_df)\n",
        "            else:\n",
        "                for word in keywords:\n",
        "                    filtered_df = df[df[colm].astype(str).str.contains(word, case=True, na=False)]\n",
        "                    # Calculate the sum of the 'logprob Confidence' column for these rows\n",
        "                    sum_logprob = filtered_df['logprob Confidence'].sum()\n",
        "                    # Update the total sum and count\n",
        "                    total_sum += sum_logprob\n",
        "                    count += len(filtered_df)\n",
        "    \n",
        "    # Calculate the average logprob\n",
        "    if count > 0:\n",
        "        return round(total_sum,2), count\n",
        "    else:\n",
        "        return None, None\n",
        "    \n",
        "    \n",
        "    \n",
        "def print_all_logprob_detections(base, category):\n",
        "    folder_path = base+\"/GPT3.5\"\n",
        "    print(\"==================== Logprob Stats for GPT 3.5 ========================\")\n",
        "    print(\"============Correct and Incorrect===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"Correct?\", [\"1\"], category)\n",
        "    if count is not None:\n",
        "        print(\"Correct questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"Correct?\", [\"0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"Incorrect Questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    print(\"============Types===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Misunderstanding context or implied background\",\"Otherwise didn't understand the goal of the question\"], category)\n",
        "    if count is not None:\n",
        "        print(\"misinterpreted questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Incorrect reasoning/deduction\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with reasoning error: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Conceptual Misunderstanding\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with conceptual error: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Wrong Information/Hallucinations\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with wrong information: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Wrong Information/Hallucinations\",\"Conceptual Misunderstanding\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with wrong information or concept: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    print(\"============Inference===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"0.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are not inferable (value = 0): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"1.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are inferable with some effort (value = 1): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"2.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are easily inferable (value = 2): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "    print(\"============Detectable===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"3.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are not detectable (value = 0): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"2.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are detectable with some effort (value = 1): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"1.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are easily detectable (value = 2): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "    folder_path = base+\"/GPT4\"\n",
        "    print(\"\\n\\n==================== Logprob Stats for GPT 4 ========================\")\n",
        "    print(\"============Correct and Incorrect===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"Correct?\", [\"1\"], category)\n",
        "    if count is not None:\n",
        "        print(\"Correct questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"Correct?\", [\"0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"Incorrect Questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))    \n",
        "    print(\"============Types===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Misunderstanding context or implied background\",\"Otherwise didn't understand the goal of the question\"], category)\n",
        "    if count is not None:\n",
        "        print(\"misinterpreted questions: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Incorrect reasoning/deduction\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with reasoning error: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Conceptual Misunderstanding\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with conceptual error: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Wrong Information/Hallucinations\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with wrong information: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"SM - Misunderstanding General\", [\"Wrong Information/Hallucinations\",\"Conceptual Misunderstanding\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions with wrong information or concept: {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    print(\"============Inference===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"0.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are not inferable (value = 0): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"2.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are easily inferable (value = 2): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"AQ - Inferrable(0-2)?\", [\"1.0\", \"2.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are inferable with some effort (value = 1 or value = 2): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "    print(\"============Detectable===============\")\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"3.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are not detectable (value = 0): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"1.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are easily detectable (value = 1): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n",
        "    total_sum, count = average_logprob_general(folder_path, \"CD - detection student(1-3)\", [\"2.0\", \"1.0\"], category)\n",
        "    if count is not None:\n",
        "        print(\"questions that are detectable with some effort (value = 1 or value = 2): {}/{} = {}%\".format(total_sum, count, round(total_sum*100/count,1)))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ECE and Logprob graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating ECE\n",
        "def calculateECE(folder, category, bins = 20):\n",
        "    LLM = read_LLM_data(folder, category)\n",
        "    n = LLM.shape[0]\n",
        "    binsize = 1/bins\n",
        "    ECE = 0\n",
        "    for i in range(bins):\n",
        "        lower_value = i*binsize\n",
        "        higher_value = (i+1)*binsize\n",
        "        filtered_df = LLM[(LLM['logprob Confidence'] > lower_value) & (LLM['logprob Confidence'] <= higher_value)]\n",
        "        B_m = filtered_df.shape[0]\n",
        "        if B_m == 0:\n",
        "            continue\n",
        "        acc = filtered_df['logprob Confidence'].mean()\n",
        "        conf = filtered_df['Score-right-minus-wrong'].mean()\n",
        "        ECE += B_m*abs(acc-conf)/n\n",
        "    return ECE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots for log prob detection\n",
        "def plot_logprob_2_graphs(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2):\n",
        "    # Sample data for the first graph\n",
        "    # x1 = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "    # percentage_answers_retained1 = [90, 85, 80, 75, 70, 65]  # Sample percentages\n",
        "    # accuracy1 = [80, 75, 70, 65, 60, 55]  # Sample accuracies\n",
        "\n",
        "    # # Sample data for the second graph\n",
        "    # x2 = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "    # percentage_answers_retained2 = [85, 80, 75, 70, 65, 60]  # Sample percentages\n",
        "    # accuracy2 = [75, 70, 65, 60, 55, 50]  # Sample accuracies\n",
        "\n",
        "    # Creating subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
        "\n",
        "    # Plotting first graph\n",
        "    ax1.plot(x1, percentage_answers_retained1, label='Answers Retained')\n",
        "    ax1.plot(x1, accuracy1, label='Accuracy')\n",
        "    plt.xlabel('Confidence Cutoff')\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    ax1.set_title('GPT-3.5')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plotting second graph\n",
        "    ax2.plot(x2, percentage_answers_retained2, label='Answers Retained')\n",
        "    ax2.plot(x2, accuracy2, label='Accuracy')\n",
        "    plt.xlabel('Confidence Cutoff')\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    ax2.set_title('GPT-4')\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.legend()\n",
        "\n",
        "    # Adjusting layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Displaying the plots\n",
        "    plt.show()\n",
        "\n",
        "# 1 is GPT3.5 and 2 is GPT4\n",
        "def plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2, filename=\"\"):\n",
        "    # Plotting\n",
        "    f = plt.figure(figsize=(5, 2))\n",
        "\n",
        "    plt.plot(x1, percentage_answers_retained1, label='Answers retained (GPT-3.5)', linestyle='-', color='salmon')\n",
        "    plt.plot(x1, accuracy1, label='Accuracy (GPT-3.5)', linestyle='--', color='salmon')\n",
        "    plt.plot(x2, percentage_answers_retained2, label='Answers retained (GPT-4)', linestyle=':', color='lightgreen')\n",
        "    plt.plot(x2, accuracy2, label='Accuracy (GPT-4)', linestyle='-.', color='lightgreen')\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel('Confidence Cutoff')\n",
        "    plt.ylabel('Percentage (%)')\n",
        "\n",
        "    # Adding legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Setting the y-axis limits between 0 and 100\n",
        "    plt.ylim(0, 100)\n",
        "\n",
        "    # Displaying the plot\n",
        "    plt.show()\n",
        "    if len(filename) > 1:\n",
        "        f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "\n",
        "# returns x, percentage_answers_retained2, accuracy\n",
        "def extract_logprob_points(folder, category, LLM_name, bins = 1000):\n",
        "    LLM = read_LLM_data(folder, category)\n",
        "    n = LLM.shape[0]\n",
        "    binsize = 1/bins\n",
        "    x = []\n",
        "    percentage_answers_retained = []\n",
        "    accuracy = []\n",
        "    lowest_diff = 100\n",
        "    lowest_diff_logprob = 0\n",
        "    lowest_diff_answers_retained = 0\n",
        "    lowest_diff_accuracy = 0\n",
        "    for i in range(bins):\n",
        "        lower_value = (i+1)*binsize\n",
        "        filtered_df = LLM[(LLM['logprob Confidence'] >= lower_value)]\n",
        "        B_m = filtered_df.shape[0]\n",
        "        if B_m == 0:\n",
        "            print(\"For {} at logprob = {}, the percentage answers retained = {} and the accuracy = {}\".format(LLM_name, round(lowest_diff_logprob,3), round(lowest_diff_answers_retained,1), round(lowest_diff_accuracy,1)))\n",
        "            return x, percentage_answers_retained, accuracy\n",
        "        x.append(lower_value)\n",
        "        percentage_answers_retained_point = B_m*100/n\n",
        "        accuracy_point = filtered_df['Score-right-minus-wrong'].mean()*100\n",
        "        percentage_answers_retained.append(percentage_answers_retained_point)\n",
        "        accuracy.append(accuracy_point)\n",
        "        if abs(percentage_answers_retained_point-accuracy_point) < lowest_diff:\n",
        "            lowest_diff = abs(percentage_answers_retained_point-accuracy_point)\n",
        "            lowest_diff_logprob = lower_value\n",
        "            lowest_diff_answers_retained = percentage_answers_retained_point\n",
        "            lowest_diff_accuracy = accuracy_point\n",
        "    print(\"For {} at logprob = {}, the percentage answers retained = {} and the accuracy = {}\".format(LLM_name, round(lowest_diff_logprob,3), round(lowest_diff_answers_retained,1), round(lowest_diff_accuracy,1)))\n",
        "    return x, percentage_answers_retained, accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sources and Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "def analyze_sources_and_explainability(folder, category):\n",
        "    total_wrong_answers = 0\n",
        "    total_sources = 0\n",
        "    total_working = 0\n",
        "    total_relevant = 0\n",
        "    total_explainable = 0\n",
        "    for file_name in os.listdir(folder):\n",
        "        if file_name.endswith('.csv') and isFileinCategory(file_name, category):\n",
        "            file_path = os.path.join(folder, file_name)\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.DictReader(file)\n",
        "                for row in reader:\n",
        "                    if row.get('Correct?') == '1':\n",
        "                        continue\n",
        "                    if row.get('AQ - Explainable?') == '1.0' or str(row.get('AQ - Explainable?')) == '1':\n",
        "                        total_explainable += 1\n",
        "                    # print(file_name)\n",
        "                    # print(row.get(\"Question Number\"))\n",
        "                    total_wrong_answers += 1\n",
        "                    source_list = ast.literal_eval(row['URLs'])\n",
        "                    # print(source_list)\n",
        "                    # print(len(source_list))\n",
        "                    total_sources += len(source_list)\n",
        "                    if len(source_list) > 0:\n",
        "                        total_working += float(row.get(\"Source links work(number)\"))\n",
        "                        if float(row.get(\"Source links work(number)\")) > 0:\n",
        "                            total_relevant += float(row.get(\"Sources Relevant(number)\"))\n",
        "    return total_wrong_answers, total_sources, total_working, total_relevant, total_explainable\n",
        "\n",
        "def print_all_source_and_explainable_info(base, category):\n",
        "    total_wrong_answers, total_sources, total_working, total_relevant, total_explainable = analyze_sources_and_explainability(base+\"/Claude3/\", category)\n",
        "    print(\"============================== Results for Claude3 ==============================\")\n",
        "    print(f\"Explainability: {total_explainable}/{total_wrong_answers} ({round(float(total_explainable)/total_wrong_answers*100,2)}%)\")\n",
        "    print(f\"Total Incorrect Answers: {total_wrong_answers}\")\n",
        "    print(f\"Total Sources in Incorrect Answers: {total_sources}\")\n",
        "    print(f\"Total Working Sources in Incorrect Answers: {total_working}/{total_sources} ({round(float(total_working)/total_sources*100,2)}%)\")\n",
        "    print(f\"Total Relevant Sources in Incorrect Answers: {total_relevant}/{total_sources} ({round(float(total_relevant)/total_sources*100,2)}%)\")\n",
        "\n",
        "    total_wrong_answers, total_sources, total_working, total_relevant, total_explainable = analyze_sources_and_explainability(base+\"/GPT4/\", category)\n",
        "    print(\"============================== Results for GPT4 ==============================\")\n",
        "    print(f\"Explainability: {total_explainable}/{total_wrong_answers} ({round(float(total_explainable)/total_wrong_answers*100,2)}%)\")\n",
        "    print(f\"Total Incorrect Answers: {total_wrong_answers}\")\n",
        "    print(f\"Total Sources in Incorrect Answers: {total_sources}\")\n",
        "    print(f\"Total Working Sources in Incorrect Answers: {total_working}/{total_sources} ({round(float(total_working)/total_sources*100,2)}%)\")\n",
        "    print(f\"Total Relevant Sources in Incorrect Answers: {total_relevant}/{total_sources} ({round(float(total_relevant)/total_sources*100,2)}%)\")\n",
        "\n",
        "    total_wrong_answers, total_sources, total_working, total_relevant, total_explainable = analyze_sources_and_explainability(base+\"/GPT3.5/\", category)\n",
        "    print(\"============================== Results for GPT3.5 ==============================\")\n",
        "    print(f\"Explainability: {total_explainable}/{total_wrong_answers} ({round(float(total_explainable)/total_wrong_answers*100,2)}%)\")\n",
        "    print(f\"Total Incorrect Answers: {total_wrong_answers}\")\n",
        "    print(f\"Total Sources in Incorrect Answers: {total_sources}\")\n",
        "    print(f\"Total Working Sources in Incorrect Answers: {total_working}/{total_sources} ({round(float(total_working)/total_sources*100,2)}%)\")\n",
        "    print(f\"Total Relevant Sources in Incorrect Answers: {total_relevant}/{total_sources} ({round(float(total_relevant)/total_sources*100,2)}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_all_llms_sources(base, category=\"all\", k=4, filename=\"\"):\n",
        "    word_frequencies_claude = count_word_frequencies(base+\"/Claude3/\", colm=\"Sources Types\", category=category)\n",
        "    word_frequencies_gpt4 = count_word_frequencies(base+\"/GPT4/\", colm=\"Sources Types\", category=category)\n",
        "    word_frequencies_gpt35 = count_word_frequencies(base+\"/GPT3.5/\", colm=\"Sources Types\", category=category)\n",
        "    plot_with_matplotlib_all_three_reasons(word_frequencies_claude, word_frequencies_gpt4, word_frequencies_gpt35, k, rotation=0, filename=filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7f1FEVZyfN"
      },
      "source": [
        "## Performance\n",
        "Table:\n",
        "-  Overall, GPT4 (88.9%) and Claude3 (89.6%) perform better than GPT3.5 (76.9%)\n",
        "-  For basic networking courses, all three LLMs have >90% performance. But GPT4 (99.0%) is better than Claude3 (94.9%) and GPT3.5 (90.9%)\n",
        "-  For higher-level courses, Claude3 (81.0%) is slightly better than GPT4 (76.2%) and much better than GPT3.5 (62.4%)\n",
        "-  For cisco-related courses, Claude3 (90.8%) and  GPT4 (89.2%) are much better than GPT3.5 (75.6%)\n",
        "-  For questions with IP addresses, Claude3 (86.0%) is better than GPT4 (79.6%) and much better than GPT3.5 (57.8%)\n",
        "\n",
        "Inferrable (i.e. someone knowledgable can extract the correct answer from explanation): GPT4 becomes slightly better - perhaps GPT4 has good concepts but can perform basic mistakes when answering\n",
        "-  Overall: GPT4 (92.1%) becomes better than Claude3 (91.2%) \n",
        "-  Cisco: GPT4 (93.9%) becomes better than Claude3 (92.9%). GPT3.5 (82.8%) improves from 75.6%\n",
        "-  IP: GPT4 (90.1%) becomes better than Claude3 (88.5%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6lNsijjZ9SF"
      },
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuGeDsrLcSR5",
        "outputId": "b2d5b528-ebfe-41c0-b602-ac24b935b0ae"
      },
      "outputs": [],
      "source": [
        "### Normal Scoring\n",
        "# printScoresAllLLMs(\"all\")\n",
        "\n",
        "### Right-minus-wrong scoring\n",
        "# printRightMinusWrongScoresAllLLMs(\"results/One-shot/\",\"all\")\n",
        "print(\"Right-minus-wrong scores (overall):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"all\")\n",
        "\n",
        "print(\"\\nRight-minus-wrong scores when counting answers that can be inferred (overall):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"all\", inference_level=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Normal Scoring\n",
        "#printScoresAllLLMs(\"Grad\")\n",
        "\n",
        "### Right-minus-wrong scoring\n",
        "print(\"Right-minus-wrong scores (for higher-level courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\")\n",
        "\n",
        "print(\"\\nRight-minus-wrong scores when counting answers that can be inferred (for higher-level courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\", inference_level=1)\n",
        "# printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\", inference_level=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Normal Scoring\n",
        "# printScoresAllLLMs(\"Basic\")\n",
        "\n",
        "### Right-minus-wrong scoring\n",
        "print(\"Right-minus-wrong scores (for basic courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\")\n",
        "print(\"\\nRight-minus-wrong scores when counting answers that can be inferred (for basic courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\", inference_level=1)\n",
        "# printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\", inference_level=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Right-minus-wrong scores (for Cisco courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\")\n",
        "print(\"\\nRight-minus-wrong scores when counting answers that can be inferred (for Cisco courses):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\", inference_level=1)\n",
        "# printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\", inference_level=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Right-minus-wrong scores (for IP related questions):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\")\n",
        "print(\"\\nRight-minus-wrong scores when counting answers that can be inferred (for IP related questions):\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\", inference_level=1)\n",
        "# printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\", inference_level=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentages'):\n",
        "def calculatePercentage(tuple_num_denom):\n",
        "    return tuple_num_denom[0]*100/tuple_num_denom[1]\n",
        "\n",
        "def createAccuracyGraph(inference = 0, detection=3, filename=\"performance.pdf\"):\n",
        "    claude3 = {}\n",
        "    claude3[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Claude3\", \"all\", inference, detection))\n",
        "    claude3[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Claude3\", \"Grad\", inference, detection))\n",
        "    claude3[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Claude3\", \"Basic\", inference, detection))\n",
        "    claude3[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Claude3\", \"Cisco\", inference, detection))\n",
        "    claude3[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/IP/Claude3\", \"all\", inference, detection))\n",
        "\n",
        "    gpt4 = {}\n",
        "    gpt4[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT4\", \"all\", inference, detection))\n",
        "    gpt4[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT4\", \"Grad\", inference, detection))\n",
        "    gpt4[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT4\", \"Basic\", inference, detection))\n",
        "    gpt4[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT4\", \"Cisco\", inference, detection))\n",
        "    gpt4[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/IP/GPT4\", \"all\", 0, detection))\n",
        "\n",
        "    gpt35 = {}\n",
        "    gpt35[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"all\", inference, detection))\n",
        "    gpt35[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Grad\", inference, detection))\n",
        "    gpt35[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Basic\", inference, detection))\n",
        "    gpt35[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Cisco\", inference, detection))\n",
        "    gpt35[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/IP/GPT3.5\", \"all\", inference, detection))\n",
        "\n",
        "    plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=5, xlabel = 'Questions', ylabel='Accuracy (%)', rotation=10, legend_loc=(0.18,0.70), bar_width=0.25, figsize=(6,3), x_label_on=True, filename=filename)\n",
        "\n",
        "createAccuracyGraph(filename=\"performance_accuracy.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "createAccuracyGraph(inference=1, filename=\"performance_inference.pdf\") # inference with detailed reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "createAccuracyGraph(detection=2, filename=\"performance_detection.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph with error bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_with_matplotlib_all_three_types_error_bars(\n",
        "    claude3, gpt4, gpt35, k=4, xlabel='Misunderstanding Types', ylabel='Percentage (%)',\n",
        "    rotation=45, legend_loc=0, min=50, labels=['Claude 3', 'GPT-4', 'GPT-3.5'],\n",
        "    bar_width=0.25, figsize=(6, 2), filename=\"performance.pdf\", x_label_on=True\n",
        "):\n",
        "    # Extracting words (keys)\n",
        "    words = list(gpt35.keys())\n",
        "    \n",
        "    # Calculating means and standard deviations for each set\n",
        "    def calc_mean_and_std(data):\n",
        "        means = [np.mean(data[key]) for key in words]\n",
        "        std_devs = [np.std(data[key]) for key in words]\n",
        "        return means, std_devs\n",
        "\n",
        "    claude3_means, claude3_std = calc_mean_and_std(claude3)\n",
        "    gpt4_means, gpt4_std = calc_mean_and_std(gpt4)\n",
        "    gpt35_means, gpt35_std = calc_mean_and_std(gpt35)\n",
        "    print(\"claude3 means:\", claude3_means)\n",
        "    print(\"gpt4 means:\", gpt4_means)\n",
        "    print(\"gpt35 means:\", gpt35_means)\n",
        "    # Set the position of the bars on the x-axis\n",
        "    r1 = np.arange(len(words))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    # Plotting word frequencies with error bars\n",
        "    f = plt.figure(figsize=figsize)\n",
        "\n",
        "    plt.bar(r1, claude3_means, yerr=claude3_std, color='skyblue', width=bar_width, \n",
        "            edgecolor='grey', label=labels[0], hatch='///', capsize=5)\n",
        "    plt.bar(r2, gpt4_means, yerr=gpt4_std, color='lightgreen', width=bar_width, \n",
        "            edgecolor='grey', label=labels[1], hatch='...', capsize=5)\n",
        "    plt.bar(r3, gpt35_means, yerr=gpt35_std, color='salmon', width=bar_width, \n",
        "            edgecolor='grey', label=labels[2], capsize=5)\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    if x_label_on:\n",
        "        plt.xlabel(xlabel, fontweight='bold')\n",
        "    plt.ylabel(ylabel, fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(words))], words, rotation=rotation)\n",
        "    plt.ylim(min, 101)\n",
        "\n",
        "    # Create legend & Show graphic\n",
        "    if legend_loc == 0:\n",
        "        plt.legend(loc=\"best\")\n",
        "    else:\n",
        "        plt.legend(loc=legend_loc)\n",
        "    # plt.title('Type of Misunderstandings')\n",
        "    plt.show()\n",
        "    if len(filename) > 1:\n",
        "        f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "# def plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentages'):\n",
        "def calculatePercentage(tuple_num_denom):\n",
        "    return tuple_num_denom[0]*100/tuple_num_denom[1]\n",
        "\n",
        "def createAccuracyGraphErrorErrorBars(inference = 0, detection=3, num_inputs = 5, filename=\"performance.pdf\"):\n",
        "    claude3 = {}\n",
        "    gpt4 = {}\n",
        "    gpt35 = {}\n",
        "    claude3[\"Overall\"] = []\n",
        "    claude3[\"Advanced\"] = []\n",
        "    claude3[\"Basic\"] = []\n",
        "    claude3[\"Cisco-Related\"] = []\n",
        "    claude3[\"Containing IP Addresses\"] = []\n",
        "\n",
        "    gpt4[\"Overall\"] = []\n",
        "    gpt4[\"Advanced\"] = []\n",
        "    gpt4[\"Basic\"] = []\n",
        "    gpt4[\"Cisco-Related\"] = []\n",
        "    gpt4[\"Containing IP Addresses\"] = []\n",
        "\n",
        "    gpt35[\"Overall\"] = []\n",
        "    gpt35[\"Advanced\"] = []\n",
        "    gpt35[\"Basic\"] = []\n",
        "    gpt35[\"Cisco-Related\"] = []\n",
        "    gpt35[\"Containing IP Addresses\"] = []\n",
        "\n",
        "    for i in range(num_inputs):\n",
        "        j = i+1\n",
        "        print(j)\n",
        "        claude3[\"Overall\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/Claude3/{}\".format(j), \"all\", inference, detection)))\n",
        "        claude3[\"Advanced\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/Claude3/{}\".format(j), \"Grad\", inference, detection)))\n",
        "        claude3[\"Basic\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/Claude3/{}\".format(j), \"Basic\", inference, detection)))\n",
        "        claude3[\"Cisco-Related\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/Claude3/{}\".format(j), \"Cisco\", inference, detection)))\n",
        "        claude3[\"Containing IP Addresses\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/IP/Claude3/{}\".format(j), \"all\", inference, detection)))\n",
        "\n",
        "        gpt4[\"Overall\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT4/{}\".format(j), \"all\", inference, detection)))\n",
        "        gpt4[\"Advanced\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT4/{}\".format(j), \"Grad\", inference, detection)))\n",
        "        gpt4[\"Basic\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT4/{}\".format(j), \"Basic\", inference, detection)))\n",
        "        gpt4[\"Cisco-Related\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT4/{}\".format(j), \"Cisco\", inference, detection)))\n",
        "        gpt4[\"Containing IP Addresses\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/IP/GPT4/{}\".format(j), \"all\", 0, detection)))\n",
        "\n",
        "        gpt35[\"Overall\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT3.5/{}\".format(j), \"all\", inference, detection)))\n",
        "        gpt35[\"Advanced\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT3.5/{}\".format(j), \"Grad\", inference, detection)))\n",
        "        gpt35[\"Basic\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT3.5/{}\".format(j), \"Basic\", inference, detection)))\n",
        "        gpt35[\"Cisco-Related\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/GPT3.5/{}\".format(j), \"Cisco\", inference, detection)))\n",
        "        gpt35[\"Containing IP Addresses\"].append(calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Multiple_runs/IP/GPT3.5/{}\".format(j), \"all\", inference, detection)))\n",
        "\n",
        "    plot_with_matplotlib_all_three_types_error_bars(claude3, gpt4, gpt35, k=5, xlabel = 'Questions', ylabel='Accuracy (%)', rotation=10, legend_loc=(0.18,0.70), bar_width=0.25, figsize=(6,3), x_label_on=True, filename=filename, min=50)\n",
        "\n",
        "createAccuracyGraphErrorErrorBars(filename=\"performance_accuracy.pdf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Errors (logprob and detection by students)\n",
        "Human detection:\n",
        "- Errors made by GPT4 and GPT3.5 are usually more easily detectable than Claude3 (e.g. Claude3 either makes more conceptual errors or is more convincing). If we remove questions that have detectable incorrect explanations: \n",
        "    -  Overall: All LLMs improve but GPT3.5 improves the most (a 6.2% increase)\n",
        "    -  In all categories, the percentage gap between Claude3 and GPT4 decreases. This shows that in practice Claude3 and GPT4 might be similar in terms of potential harms\n",
        "\n",
        "Confidence: (only for GPT4 and GPT3.5) (need help with this)\n",
        "- Overall: The average confidence of answers for incorrect answers is much lower than for correct answers\n",
        "    - GPT3.5 has low confidence when it makes conceptual recall errors\n",
        "    - GPT4 has low confidence for both conceptual/informational recall errors and misinterpreted questions\n",
        "    - Reasoning errors are harder to detect using confidence\n",
        "- Overall: Errors that are detectable by humans are not necessarily the ones that LLMs are less confident aboutLLMs have almost the same amount of confidence on errors that are detectable by humans as errors that are not \n",
        "    - This might be good, since now we have two orthogonal ways to detect errors \n",
        "- Higher-level courses: GPT3.5 overall has less confidence in answers (i.e. is more unsure)\n",
        "\n",
        "ECE Score:\n",
        "- Overall: GPT4 (0.086) is better calibrated than GPT3.5 (0.169)\n",
        "- Higher-Level Courses: Both GPT4 (0.192) and GPT3.5 (0.250) are poorly calibrated\n",
        "- IP-related Questions: Both GPT4 (0.167) and GPT3.5 (0.349) are poorly calibrated\n",
        "\n",
        "Logprob graphs: A way to find how to rely on model confidence to weed out potentially incorrect answers\n",
        "- Overall: If we only consider questions with confidence 86% or higher, we retain 92.2% questions with around 92.2% accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresAllLLMs(\"results\",\"all\", detection=3)\n",
        "print(\"\\nResults if we remove incorrect questions that are quickly detectable\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"all\", detection=1)\n",
        "print(\"\\nResults if we remove incorrect questions that are detectable with detailed reading\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"all\", detection=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_logprob_detections(\"results\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ECE_35 = calculateECE(\"Results/GPT3.5\",\"all\", bins=10)\n",
        "print(\"ECE Score for GPT3.5:\", round(ECE_35,3))\n",
        "ECE_4 = calculateECE(\"Results/GPT4\",\"all\", bins=10)\n",
        "print(\"ECE Score for GPT4:\", round(ECE_4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1, percentage_answers_retained1, accuracy1 = extract_logprob_points(\"Results/GPT3.5/\",\"all\", \"GPT3.5\")\n",
        "x2, percentage_answers_retained2, accuracy2 = extract_logprob_points(\"Results/GPT4/\",\"all\", \"GPT4\")\n",
        "plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2, filename=\"logprob.pdf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\", detection=3)\n",
        "print(\"\\nResults if we remove incorrect questions that are quickly detectable\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\", detection=1)\n",
        "print(\"\\nResults if we remove incorrect questions that are detectable with detailed reading\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Grad\", detection=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_logprob_detections(\"results\",\"Grad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ECE_35 = calculateECE(\"Results/GPT3.5\",\"Grad\", bins=10)\n",
        "print(\"ECE Score for GPT3.5:\", round(ECE_35,3))\n",
        "ECE_4 = calculateECE(\"Results/GPT4\",\"Grad\", bins=10)\n",
        "print(\"ECE Score for GPT4:\", round(ECE_4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1, percentage_answers_retained1, accuracy1 = extract_logprob_points(\"Results/GPT3.5/\",\"Grad\", \"GPT3.5\")\n",
        "x2, percentage_answers_retained2, accuracy2 = extract_logprob_points(\"Results/GPT4/\",\"Grad\", \"GPT4\")\n",
        "plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\", detection=3)\n",
        "print(\"\\nResults if we remove incorrect questions that are quickly detectable\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\", detection=1)\n",
        "print(\"\\nResults if we remove incorrect questions that are detectable with detailed reading\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Basic\", detection=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_logprob_detections(\"results\",\"Basic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ECE_35 = calculateECE(\"Results/GPT3.5\",\"Basic\", bins=10)\n",
        "print(\"ECE Score for GPT3.5:\", round(ECE_35,3))\n",
        "ECE_4 = calculateECE(\"Results/GPT4\",\"Basic\", bins=10)\n",
        "print(\"ECE Score for GPT4:\", round(ECE_4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1, percentage_answers_retained1, accuracy1 = extract_logprob_points(\"Results/GPT3.5/\",\"Basic\", \"GPT3.5\")\n",
        "x2, percentage_answers_retained2, accuracy2 = extract_logprob_points(\"Results/GPT4/\",\"Basic\", \"GPT4\")\n",
        "plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\", detection=3)\n",
        "print(\"\\nResults if we remove incorrect questions that are quickly detectable\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\", detection=1)\n",
        "print(\"\\nResults if we remove incorrect questions that are detectable with detailed reading\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results\",\"Cisco\", detection=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_logprob_detections(\"results\",\"Cisco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ECE_35 = calculateECE(\"Results/GPT3.5\",\"Cisco\", bins=10)\n",
        "print(\"ECE Score for GPT3.5:\", round(ECE_35,3))\n",
        "ECE_4 = calculateECE(\"Results/GPT4\",\"Cisco\", bins=10)\n",
        "print(\"ECE Score for GPT4:\", round(ECE_4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1, percentage_answers_retained1, accuracy1 = extract_logprob_points(\"Results/GPT3.5/\",\"Cisco\", \"GPT3.5\")\n",
        "x2, percentage_answers_retained2, accuracy2 = extract_logprob_points(\"Results/GPT4/\",\"Cisco\", \"GPT4\")\n",
        "plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\", detection=3)\n",
        "print(\"\\nResults if we remove incorrect questions that are quickly detectable\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\", detection=1)\n",
        "print(\"\\nResults if we remove incorrect questions that are detectable with detailed reading\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\", detection=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_logprob_detections(\"results/IP\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ECE_35 = calculateECE(\"Results/IP/GPT3.5\",\"all\", bins=10)\n",
        "print(\"ECE Score for GPT3.5:\", round(ECE_35,3))\n",
        "ECE_4 = calculateECE(\"Results/IP/GPT4\",\"all\", bins=10)\n",
        "print(\"ECE Score for GPT4:\", round(ECE_4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1, percentage_answers_retained1, accuracy1 = extract_logprob_points(\"Results/IP/GPT3.5/\",\"all\", \"GPT3.5\")\n",
        "x2, percentage_answers_retained2, accuracy2 = extract_logprob_points(\"Results/IP/GPT4/\",\"all\", \"GPT4\")\n",
        "plot_logprob(x1, percentage_answers_retained1, accuracy1, x2, percentage_answers_retained2, accuracy2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effects (Misconceptions and subtopics)\n",
        "Notable Results:\n",
        "- Overall: Although GPT3.5 has worse overall performance, its explanations are less likely to cause misconceptions compared to Claude3 and GPT4\n",
        "- GPT3.5 makes a lot of errors related to basic networking concepts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printMisconceptions(\"Results/\", \"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/\", \"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/\", \"all\", misconceptionsCheck=True, filename=\"effects_subtopics.pdf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printMisconceptions(\"Results/\", \"Grad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/\", \"Grad\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printMisconceptions(\"Results/\", \"Basic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/\", \"Basic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printMisconceptions(\"Results/\", \"Cisco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/\", \"Cisco\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printMisconceptions(\"Results/IP\", \"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_subtopics(\"Results/IP\", \"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type of misunderstanding (primary types and reasons)\n",
        "Notable results:\n",
        "-  Overall: Most errors are based on information recall or concpetual issues. GPT4 is slightly better at conceptual/informational recall\n",
        "-  High-level networking courses: GPT3.5 makes a high percentage of conceptual/informational recall errors (e.g. either GPT3.5 has not been exposed to the appropriate data or lacks the capability to recall rarer information)\n",
        "-  Most of the mistakes that GPT4 and GPT3.5 make in IP related questions are related to incorrect handling of IP addresses"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_types(base=\"Results/\", category=\"all\", filename=\"misunderstanding_types.pdf\")\n",
        "plot_all_llms_reasons(base=\"Results/\", category=\"all\", k=3, ylim = 100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_types(base=\"Results/\", category=\"Grad\")\n",
        "plot_all_llms_reasons(base=\"Results/\", category=\"Grad\", k=4, ylim = 60)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_types(base=\"Results/\", category=\"Basic\")\n",
        "plot_all_llms_reasons(base=\"Results/\", category=\"Basic\", k=4, ylim=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_types(base=\"Results/\", category=\"Cisco\")\n",
        "plot_all_llms_reasons(base=\"Results/\", category=\"Cisco\", k=4, ylim=40)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_types(base=\"Results/IP\", category=\"all\")\n",
        "plot_all_llms_reasons(base=\"Results/IP\", category=\"all\", k=4, ylim=17, filename=\"misunderstanding_reasons.pdf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explainability (sources and explainability score)\n",
        "Notable Results (Explainability):\n",
        "- Overall: GPT3.5 (68.79%) scores low on explainability while GPT4 (91.25%) scores higher than Claude3 (84.51%)\n",
        "\n",
        "Notable Results (Sources):\n",
        "- Overall: Almost 1/3rd sources given by GPT4 and Claude3 are not working, and almost half are irrelevant\n",
        "- Overall: GPT3.5 provides very limited sources\n",
        "- Cisco: GPT4 (80.95%) provides much more working and relevant sources (maybe it does have cisco related documentation as part of its training set?)\n",
        "- (Not shown): Claude3 mostly provided books as sources, while GPT4 provided direct links to articles, documentation, wikipedia links which might be more convenient\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_source_and_explainable_info(\"Results\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_sources(\"Results\",\"all\",k=4, filename=\"sources.pdf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_source_and_explainable_info(\"Results\",\"Grad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_sources(\"Results\",\"Grad\",k=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_source_and_explainable_info(\"Results\",\"Basic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_sources(\"Results\",\"Basic\",k=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_source_and_explainable_info(\"Results\",\"Cisco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_llms_sources(\"Results\",\"Cisco\",k=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_source_and_explainable_info(\"Results/IP\",\"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b>Improvement strategies<b>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best of 2/3\n",
        "Maybe also take into account the confidence i.e. if the confidence of current LLM is low, consult another LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Overall\")\n",
        "calculate_best_out_of_3_score(\"results/\",\"all\")\n",
        "print(\"\\nHigh-level courses\")\n",
        "calculate_best_out_of_3_score(\"results/\",\"Grad\")\n",
        "print(\"\\nBasic courses\")\n",
        "calculate_best_out_of_3_score(\"results/\",\"Basic\")\n",
        "print(\"\\nCisco-related courses\")\n",
        "calculate_best_out_of_3_score(\"results/\",\"Cisco\")\n",
        "print(\"\\nIP Related\")\n",
        "calculate_best_out_of_3_score(\"results/IP\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Overall\")\n",
        "printScoreDifferenceFromMajorityVoting(\"results/\",\"all\")\n",
        "print(\"\\nHigh-level courses\")\n",
        "printScoreDifferenceFromMajorityVoting(\"results/\",\"Grad\")\n",
        "print(\"\\nBasic courses\")\n",
        "printScoreDifferenceFromMajorityVoting(\"results/\",\"Basic\")\n",
        "print(\"\\nCisco-related courses\")\n",
        "printScoreDifferenceFromMajorityVoting(\"results/\",\"Cisco\")\n",
        "print(\"\\nIP Related\")\n",
        "printScoreDifferenceFromMajorityVoting(\"results/IP\",\"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-shot "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"all\")\n",
        "print(\"\\nOne-shot scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"Results/One-shot/\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/One-shot/\", \"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Grad\")\n",
        "print(\"\\nOne-shot scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"Results/One-shot/\",\"Grad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/One-shot/\", \"Grad\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Basic\")\n",
        "print(\"\\nOne-shot scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"Results/One-shot/\",\"Basic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/One-shot/\", \"Basic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Cisco\")\n",
        "print(\"\\nOne-shot scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"Results/One-shot/\",\"Cisco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/One-shot/\", \"Cisco\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\")\n",
        "print(\"\\nOne-shot scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"Results/One-shot/IP\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/IP\", \"results/One-shot/IP\", \"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-checking\n",
        "Notable Results:\n",
        "- Overall, self-checking actually degrades performance\n",
        "- However, for questions based on IP addresses, performance improves significantly for both GPT4 (from 79.6% to 84.5%) and GPT3.5 (from 57.8% to 69.8%)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"all\")\n",
        "print(\"\\nSelf-corrected scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/self-correction/\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/self-correction/\", \"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Grad\")\n",
        "print(\"\\nSelf-corrected scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/self-correction/\",\"Grad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/self-correction/\", \"Grad\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Basic\")\n",
        "print(\"\\nSelf-corrected scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/self-correction/\",\"Basic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/self-correction/\", \"Basic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/\",\"Cisco\")\n",
        "print(\"\\nSelf-corrected scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/self-correction/\",\"Cisco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/\", \"results/self-correction/\", \"Cisco\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Old Scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/IP\",\"all\")\n",
        "print(\"\\nSelf-corrected scores:\")\n",
        "printRightMinusWrongScoresAllLLMs(\"results/self-correction/IP\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printRightMinusWrongScoresDifference(\"results/IP\", \"results/self-correction/IP\", \"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <b>Correlations<b>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normal Answers\n",
        "Notable Results:\n",
        "- Overall there is little correlation between answers between the LLMs\n",
        "- For Cisco related questions, there is moderate correlation between GPT3.5 and Claude3. This might hint on similar training data for Cisco related text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_normal_case(\"Results\",\"all\")\n",
        "print_all_contingency_tables_normal_case(\"Results\",\"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_normal_case(\"Results\",\"Grad\")\n",
        "print_all_contingency_tables_normal_case(\"Results\",\"Grad\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_normal_case(\"Results\",\"Basic\")\n",
        "print_all_contingency_tables_normal_case(\"Results\",\"Basic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_normal_case(\"Results\",\"Cisco\")\n",
        "print_all_contingency_tables_normal_case(\"Results\",\"Cisco\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_normal_case(\"Results/IP\",\"all\")\n",
        "print_all_contingency_tables_normal_case(\"Results/IP\",\"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Randomized Choices (Robustness)\n",
        "Notable Results: Note that this would depend on the temperature and the confidence\n",
        "- Overall: Moderate (not high) correlations between original answers and answers with reordered choices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Across all topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_reordered_case(\"Results\",\"Results/Reordered/\",\"all\")\n",
        "print_all_contingency_tables_reordered_case(\"Results\",\"Results/Reordered/\",\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Differences when temp=0\n",
        "LLM1_correct, LLM2_correct, both_correct, both_incorrect = contingency_table(\"results/Temp_0/\", \"results/Temp_0_reordered/\", \"all\")\n",
        "matrix = [[both_correct,LLM2_correct],[LLM1_correct,both_incorrect]]\n",
        "headers = [\"GPT3.5 Correct\",\"GPT3.5 Incorrect\"]\n",
        "index = [\"GPT3.5 Reordered Correct\",\"GPT3.5 Reordered Incorrect\"]\n",
        "num_ques = both_correct+LLM1_correct+LLM2_correct+both_incorrect\n",
        "diff = LLM1_correct+LLM2_correct\n",
        "print(tabulate(matrix, headers=headers, showindex=index))\n",
        "print(\"Difference in {}/{} ({}%) questions\".format(diff, num_ques, round(diff*100/num_ques,2)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Higher-level Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_reordered_case(\"Results\",\"Results/Reordered/\",\"Grad\")\n",
        "print_all_contingency_tables_reordered_case(\"Results\",\"Results/Reordered/\",\"Grad\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_reordered_case(\"Results\",\"Results/Reordered/\",\"Basic\")\n",
        "print_all_contingency_tables_reordered_case(\"Results\",\"Results/Reordered/\",\"Basic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cisco Related Computer Networking Courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_reordered_case(\"Results\",\"Results/Reordered/\",\"Cisco\")\n",
        "print_all_contingency_tables_reordered_case(\"Results\",\"Results/Reordered/\",\"Cisco\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions related to IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_all_correlations_reordered_case(\"Results/IP\",\"Results/Reordered/IP\",\"all\")\n",
        "print_all_contingency_tables_reordered_case(\"Results/IP\",\"Results/Reordered/IP\",\"all\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion section\n",
        "- Some sort of takeaways (e.g. lessons for teachers, lessons for future)\n",
        "- Ways to leverage these tools? Possible ramifications without saying too binding\n",
        "    - e.g. might be good to use multiple engines since they have some orthogonaly (similar to inversion programming)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open-source LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_with_matplotlib_all_four_types(llm1, llm2, llm3, llm4, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentage (%)', rotation=45, legend_loc=0, min=50, labels=['Claude 3','GPT-4','GPT-3.5','llm4'], filename=\"open-source.pdf\"):\n",
        "    # Extracting words and their frequencies for each set of word frequencies\n",
        "    words = list(llm1.keys())\n",
        "\n",
        "    # Set the width of the bars\n",
        "    bar_width = 0.2\n",
        "\n",
        "    # Set the position of the bars on the x-axis\n",
        "    r1 = np.arange(len(words))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "    r4 = [x + bar_width for x in r3]\n",
        "\n",
        "    # Plotting word frequencies side by side\n",
        "    f = plt.figure(figsize=(7, 2))\n",
        "\n",
        "    plt.bar(r1, list(llm1.values()), color='skyblue', width=bar_width, edgecolor='grey', label=labels[0], hatch= '///')\n",
        "    plt.bar(r2, list(llm2.values()), color='lightgreen', width=bar_width, edgecolor='grey', label=labels[1],  hatch= '...')\n",
        "    plt.bar(r3, list(llm3.values()), color='blue', width=bar_width, edgecolor='grey', label=labels[2], hatch= '***')\n",
        "    plt.bar(r4, list(llm4.values()), color='salmon', width=bar_width, edgecolor='grey', label=labels[3])\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.xlabel(xlabel, fontweight='bold')\n",
        "    plt.ylabel(ylabel, fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(words))], words, rotation=rotation)\n",
        "    plt.ylim(min, 101) \n",
        "\n",
        "    # Create legend & Show graphic\n",
        "    if legend_loc == 0:\n",
        "        plt.legend(loc=\"best\")\n",
        "    else:\n",
        "        plt.legend(loc=legend_loc)\n",
        "    # plt.title('Type of Misunderstandings')\n",
        "    plt.show()\n",
        "    f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "# def plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentages'):\n",
        "def createAccuracyGraphOpen(inference = 0, detection=3):\n",
        "    mistral = {}\n",
        "    mistral[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/mistral\", \"all\", inference, detection))\n",
        "    mistral[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/mistral\", \"Grad\", inference, detection))\n",
        "    mistral[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/mistral\", \"Basic\", inference, detection))\n",
        "    mistral[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/mistral\", \"Cisco\", inference, detection))\n",
        "    mistral[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/IP/mistral\", \"all\", inference, detection))\n",
        "\n",
        "    llama3 = {}\n",
        "    llama3[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/llama3.1\", \"all\", inference, detection))\n",
        "    llama3[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/llama3.1\", \"Grad\", inference, detection))\n",
        "    llama3[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/llama3.1\", \"Basic\", inference, detection))\n",
        "    llama3[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/llama3.1\", \"Cisco\", inference, detection))\n",
        "    llama3[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/IP/llama3.1\", \"all\", inference, detection))\n",
        "\n",
        "    gemma2 = {}\n",
        "    gemma2[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/gemma2\", \"all\", inference, detection))\n",
        "    gemma2[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/gemma2\", \"Grad\", inference, detection))\n",
        "    gemma2[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/gemma2\", \"Basic\", inference, detection))\n",
        "    gemma2[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/gemma2\", \"Cisco\", inference, detection))\n",
        "    gemma2[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/open-source/IP/gemma2\", \"all\", 0, detection))\n",
        "\n",
        "    gpt35 = {}\n",
        "    gpt35[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"all\", inference, detection))\n",
        "    gpt35[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Grad\", inference, detection))\n",
        "    gpt35[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Basic\", inference, detection))\n",
        "    gpt35[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Cisco\", inference, detection))\n",
        "    gpt35[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/IP/GPT3.5\", \"all\", inference, detection))\n",
        "\n",
        "    plot_with_matplotlib_all_four_types(mistral, llama3, gemma2, gpt35, k=5, xlabel = 'Questions', ylabel='Accuracy (%)', rotation=10, legend_loc=(0.78,0.41), labels=[\"Mistral\", \"Llama-3.1\", \"Gemma2\", \"GPT-3.5\"], min=30)\n",
        "\n",
        "createAccuracyGraphOpen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Difference Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Define the valid values for each column\n",
        "valid_values = {\n",
        "    \"SM - Misunderstanding General\": [\"Wrong Facts/Concept\", \"Misinterpreting questions\", \"Incorrect reasoning/deduction\"],\n",
        "    \"SM - Misunderstanding Reasons\": [\"Corner Case\", \"Out of date information\", \"Incorrect information/concept\", \"Quantifier issue\", \"Direct vs Indirect Causation\", \"Misinterpreting a word\", \"Incorrect copying of the question\", \"No explanation given\", \"Incorrect calculation or counting\", \"An error related to misinterpreting IP addresses\", \"Contradictory reasoning\", \"Senseless\", \"Faulty inference\", \"Self-aware but still wrong conclusion\", \"Incorrect Choice\"],\n",
        "    \"AQ - Inferrable(0-2)?\" : [0, 1, 2],\n",
        "    # \"AQ - Precise?\" : [0, 1],\n",
        "    \"AQ - Explainable?\" : [0, 1],\n",
        "    \"Effect - Conceptual error in explanaiton?(0/1)\" : [0, 1],\n",
        "    \"CD - detection student(1-3)\" : [1, 2, 3]\n",
        "    # \"CD - correction student(1-8)\" : [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    # \"CD - detection knowledgeable(1-3)\" : [1, 2, 3],\n",
        "    # \"CD - correction knowledgeable(1-8)\" : [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "}\n",
        "\n",
        "def load_and_concat_files(folder, valid_values):\n",
        "    all_data = []\n",
        "    \n",
        "    # Iterate through all files in the folder\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            df = pd.read_csv(filepath)\n",
        "            if df.empty:\n",
        "                # print(\"Empty file. Skipping:\", filename)\n",
        "                continue\n",
        "            # Add filename as a new column to ensure uniqueness when merging\n",
        "            df['filename'] = filename\n",
        "            \n",
        "            # Only keep rows with valid \"Question Number\"\n",
        "            df = df[df['Question Number'].notna()]\n",
        "            \n",
        "            # Convert float columns to int where necessary\n",
        "            for column in valid_values.keys():\n",
        "                if column in df.columns and pd.api.types.is_float_dtype(df[column]):\n",
        "                    df[column] = df[column].astype(int)\n",
        "                    # df[column] = df[column].astype(str)\n",
        "            \n",
        "            # Append the data to the list\n",
        "            all_data.append(df)\n",
        "    \n",
        "    # Concatenate all data into a single DataFrame\n",
        "    concatenated_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    return concatenated_df\n",
        "\n",
        "def calculate_kappa_across_files(folder1, folder2, valid_values):\n",
        "    # Load and concatenate all files from both folders\n",
        "    df1 = load_and_concat_files(folder1, valid_values)\n",
        "    df2 = load_and_concat_files(folder2, valid_values)\n",
        "\n",
        "    \n",
        "    # Merge on \"Question Number\" and \"filename\" to align the rows for comparison\n",
        "    merged_df = pd.merge(df1, df2, on=[\"Question Number\", \"filename\"], suffixes=('_1', '_2'))\n",
        "    \n",
        "    # Iterate through each column in the valid_values dictionary\n",
        "    for column, valid in valid_values.items():\n",
        "        col_1 = column + '_1'\n",
        "        col_2 = column + '_2'\n",
        "        if col_1 in merged_df.columns and col_2 in merged_df.columns:\n",
        "            # Validate values in both columns\n",
        "            df_valid = merged_df[merged_df[col_1].isin(valid) & merged_df[col_2].isin(valid)]\n",
        "            # print(df_valid[col_1])\n",
        "            # print(df_valid[col_2])\n",
        "            # print(column)\n",
        "            \n",
        "            # Calculate Cohen's Kappa score\n",
        "            kappa_score = cohen_kappa_score(df_valid[col_1], df_valid[col_2])\n",
        "            print(f\"Cohen's Kappa for '{column}': {kappa_score:.4f}\")\n",
        "        else:\n",
        "            print(f\"Warning: '{column}' column is missing in the merged data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calculate_kappa_across_files(\"Results/Labels/Labeller1\", \"Results/Labels/Labeller2\", valid_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_with_matplotlib_all_four_types(llm1, llm2, llm3, llm4, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentage (%)', rotation=45, legend_loc=0, min=50, labels=['Claude 3','GPT-4','GPT-3.5','llm4'], filename=\"finetuned.pdf\"):\n",
        "    # Extracting words and their frequencies for each set of word frequencies\n",
        "    words = list(llm1.keys())\n",
        "\n",
        "    # Set the width of the bars\n",
        "    bar_width = 0.2\n",
        "\n",
        "    # Set the position of the bars on the x-axis\n",
        "    r1 = np.arange(len(words))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "    r4 = [x + bar_width for x in r3]\n",
        "\n",
        "    # Plotting word frequencies side by side\n",
        "    f = plt.figure(figsize=(7, 3))\n",
        "\n",
        "    plt.bar(r1, list(llm1.values()), color='skyblue', width=bar_width, edgecolor='grey', label=labels[0], hatch= '///')\n",
        "    plt.bar(r2, list(llm2.values()), color='lightgreen', width=bar_width, edgecolor='grey', label=labels[1],  hatch= '...')\n",
        "    plt.bar(r3, list(llm3.values()), color='blue', width=bar_width, edgecolor='grey', label=labels[2], hatch= '***')\n",
        "    plt.bar(r4, list(llm4.values()), color='salmon', width=bar_width, edgecolor='grey', label=labels[3])\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.xlabel(xlabel, fontweight='bold')\n",
        "    plt.ylabel(ylabel, fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(words))], words, rotation=rotation)\n",
        "    plt.ylim(min, 101) \n",
        "\n",
        "    # Create legend & Show graphic\n",
        "    if legend_loc == 0:\n",
        "        plt.legend(loc=\"best\")\n",
        "    else:\n",
        "        plt.legend(loc=legend_loc)\n",
        "    # plt.title('Type of Misunderstandings')\n",
        "    plt.show()\n",
        "    f.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "# def plot_with_matplotlib_all_three_types(claude3, gpt4, gpt35, k=4, xlabel = 'Misunderstanding Types', ylabel='Percentages'):\n",
        "def createAccuracyGraphFine(inference = 0, detection=3):\n",
        "    large = {}\n",
        "    large[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_full\", \"all\", inference, detection))\n",
        "    large[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_full\", \"Grad\", inference, detection))\n",
        "    large[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_full\", \"Basic\", inference, detection))\n",
        "    large[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_full\", \"Cisco\", inference, detection))\n",
        "    large[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/IP/Finetuned_full\", \"all\", inference, detection))\n",
        "\n",
        "    small = {}\n",
        "    small[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_small\", \"all\", inference, detection))\n",
        "    small[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_small\", \"Grad\", inference, detection))\n",
        "    small[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_small\", \"Basic\", inference, detection))\n",
        "    small[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_small\", \"Cisco\", inference, detection))\n",
        "    small[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/IP/Finetuned_small\", \"all\", inference, detection))\n",
        "\n",
        "    ip_ft = {}\n",
        "    ip_ft[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_ip\", \"all\", inference, detection))\n",
        "    ip_ft[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_ip\", \"Grad\", inference, detection))\n",
        "    ip_ft[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_ip\", \"Basic\", inference, detection))\n",
        "    ip_ft[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/Finetuned_ip\", \"Cisco\", inference, detection))\n",
        "    ip_ft[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/Finetuning_results/IP/Finetuned_ip\", \"all\", 0, detection))\n",
        "\n",
        "    gpt35 = {}\n",
        "    gpt35[\"Overall\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"all\", inference, detection))\n",
        "    gpt35[\"Advanced\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Grad\", inference, detection))\n",
        "    gpt35[\"Basic\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Basic\", inference, detection))\n",
        "    gpt35[\"Cisco-Related\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/GPT3.5\", \"Cisco\", inference, detection))\n",
        "    gpt35[\"Containing IP Addresses\"] = calculatePercentage(calculate_total_score_Right_minus_wrong(\"Results/IP/GPT3.5\", \"all\", inference, detection))\n",
        "\n",
        "    plot_with_matplotlib_all_four_types(ip_ft, small, large, gpt35, k=5, xlabel = 'Questions', ylabel='Accuracy (%)', rotation=10, legend_loc=(0.1,0.72), labels=[\"GPT-3.5-ft-ip\", \"GPT-3.5-ft-small\", \"GPT-3.5-ft-large\", \"GPT-3.5\"], min=30)\n",
        "\n",
        "createAccuracyGraphFine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
