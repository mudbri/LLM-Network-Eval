Question Number,Question Text,Choices,Valid Question?,Correct Answer,LLM Answer,Correct Explanation,LLM Explanation,URLs,Correct?,Topic,LLM Confidence,logprob Confidence
113,"Which are reasonable approaches for designing a fast, scalable, programmable data plane? Select all that apply.","a. Make hardware routers more programmable with custom OpenFlow chips.
b. Make software routers faster by reducing the need to copy packets from the kernel into user space.
c. Send all traffic through software switches running in user space.
d. Make software routers faster by distributing software routers across clusters.
e. Make software routers faster by amortizing packet-processing costs across batches of packets.
",,"a,b,d,e","a,b,d,e",,"a) Programmable hardware (programmable match‑action ASICs / custom chips that expose OpenFlow/P4-like match/action primitives) is a standard approach to get both line-rate performance and flexibility. b) Eliminating kernel↔user copies (zero‑copy I/O, DPDK/netmap/PF_RING) significantly improves software router throughput and is widely used. c) Routing all traffic purely via unaccelerated user‑space switches is generally not practical at high line rates — it creates a bottleneck unless combined with hardware offload or zero‑copy/batching techniques. d) Distributing packet processing across a cluster (scale‑out / NFV) is a common way to increase capacity, though it requires state partitioning and synchronization. e) Batching packets amortizes per‑packet overhead (system calls, DMA setup, cache misses) and is a common performance optimization in fast software dataplanes. Consequently a, b, d, and e are reasonable approaches; c alone is not. ","['https://conferences.sigcomm.org/sigcomm/2013/papers/p73-bosshart.pdf (Bosshart et al., RMT/programmable match-action hardware)', 'https://p4.org (P4 language and programmable data plane concepts)', 'https://www.dpdk.org (Data Plane Development Kit — zero-copy & burst/batching APIs)', 'https://www.netmap.org/ (netmap zero-copy packet I/O)', 'https://pdos.csail.mit.edu/papers/click.pdf (Click modular router — software packet processing and scaling concepts)', 'https://ieeexplore.ieee.org/document/7066076 (Kreutz et al., Software-Defined Networking survey; discussion of scaling/NFV)']",1,10_programmable_data_planes,0.92,1
114,What are functions that are commonly performed by a programmable data plane? Select all that apply.,"a. Performing deep packet inspection
b. Forwarding
c. Specifying access control policies
d. Shaping traffic
e. Computing routes
",,"a,b,d","b,c,d",,"The data plane’s role is to process and handle packets (forwarding) and to enforce packet-level policies implemented as match‑action logic — e.g., access control lists (ACLs) — and performance controls such as metering/queuing and rate shaping. Route computation (finding paths) is a control‑plane function, not normally done in the data plane. Deep packet inspection (full payload/application‑layer analysis) is typically performed by specialized middleboxes or software; while some programmable data planes can do limited payload matching, full DPI is not a common/core data‑plane function.","['https://datatracker.ietf.org/doc/html/rfc7426', 'https://opennetworking.org/wp-content/uploads/2014/10/wp-sdn-newnorm.pdf', 'https://p4.org/', 'https://opennetworking.org/wp-content/uploads/2013/04/OpenFlow-spec-v1.5.1.pdf']",0,10_programmable_data_planes,0.9,1
115,Which of the following functions might be performed by a Click element? Select all that apply.,"a. Reading a packet from a network device
b. Dropping packets
c. Counting packets
d. Classifying packets into different priority queues
e. Computing shortest paths routes between two routers
",,"a,b,c,d","a,b,c,d",,"Click elements implement packet-processing primitives. Examples include FromDevice (reading a packet from a network device), Drop (dropping packets), Count/Counter elements (counting packets), and Classifier/Queue elements (classifying packets into queues or implementing priority queues). Computing shortest-path routes between routers is a control-plane/routing-algorithm function and not a typical per-packet Click element responsibility.","['http://pdos.csail.mit.edu/papers/click.pdf', 'http://read.cs.ucla.edu/click/', 'http://read.cs.ucla.edu/click/click/elements/']",1,10_programmable_data_planes,0.95,1
116,Which of the following are true about programmable data planes in Click? Select all that apply.,"a. Click elements cannot perform common data plane operations such as TTL decrement or checksum computations.
b. Data-plane processing in Click is limited to a fixed set of elements provided by the software.
c. Click elements can be composed to perform complex functions, such as the forwarding at an IP router.
d. Click configuration is a simplified version of Python.
e. Click elements can be composed to perform complex network-wide functions, such as shortest paths or BGP routing.
",,c,c,,"Only (c) is correct. Click is a modular, extensible packet-processing framework whose elements are composed to build complex per-node packet-processing functions (for example, IP-router forwarding pipelines). (a) is false — Click elements do perform common packet ops (e.g., TTL decrement, checksum) and users can write elements for such tasks. (b) is false — the set of elements is extensible (users implement new elements in C++). (d) is false — Click uses its own configuration language/DSL, not a simplified Python. (e) is false as stated — Click composes data‑plane elements for per-node packet processing; network‑wide functions like computing shortest paths or running BGP are distributed control‑plane algorithms and are not provided simply by composing local data‑plane elements (although one can implement routing-related behavior, full network‑wide control requires additional control‑plane logic/protocols).","['https://pdos.csail.mit.edu/papers/click-sosp2000.pdf', 'https://click.csail.mit.edu/', 'https://click.csail.mit.edu/doc/']",1,10_programmable_data_planes,0.95,1
117,Why does a RouteBricks server assign one core per queue?,"a. Having multiple cores accessing the same queue would require each core to perform only one function in the packet processing pipeline, which is inefficient.
b. Having multiple cores accessing the same queue would require locking to prevent concurrent memory accesses, slowing packet forwarding performance.
c. Having multiple cores accessing the same queue might create the possibility that there is a queue that is not served by any cores.
d. Cores have heterogeneous performance, and packets are different sizes, so it makes sense to assign less powerful cores to queues that have smaller packets.
",,b,b,,"RouteBricks maps one core to one NIC queue to avoid concurrent access to the same queue by multiple cores. If multiple cores accessed the same queue they would need synchronization (locks or atomic operations), causing cache-line contention, lock overhead and degraded packet-forwarding performance. The one-core-per-queue design preserves cache locality and lock-free packet paths for high throughput and low latency.","['https://pdos.csail.mit.edu/routebricks/ (RouteBricks project/paper)', 'https://www.dpdk.org/ (Data Plane Development Kit documentation on poll-mode drivers and core-to-queue mapping)', 'https://www.intel.com/content/www/us/en/developer/articles/technical/receive-side-scaling.html (Intel: Receive Side Scaling and multi-queue NICs)']",1,10_programmable_data_planes,0.9,1
118,What are some tricks commonly employed in software to accelerate packet forwarding? Select all that apply.,"a. Ethernet GRE tunnels
b. Large packet buffers that hold multiple packets that can be read with a single read
c. Forwarding smaller packets before larger packets
d. Avoiding lookups on the software bridge between virtual interfaces and physical interfaces
e. Batch processing
",,"a,b,d,e","b,d,e",,"Common software techniques to speed packet forwarding include: (b) using larger packet buffers or techniques like GRO/LRO/GSO and recvmmsg/mmsg so multiple packets can be read/processed with fewer system calls; (e) batching packets for rx/tx and processing them together to amortize per-packet overhead; and (d) avoiding per-packet software lookups on the slow path (e.g., using flow caches, fast-path shortcuts or kernel-bypass/vhost/SR‑IOV) so most packets follow a cheap fast path. Option (a) (Ethernet GRE tunnels) is an encapsulation mechanism, not a general software acceleration trick, and (c) prioritizing smaller packets over larger ones is a scheduling policy that may change latency distribution but is not a common general-purpose forwarding acceleration technique.","['https://www.kernel.org/doc/html/latest/networking/napi.html', 'https://man7.org/linux/man-pages/man2/recvmmsg.2.html', 'https://www.kernel.org/doc/html/latest/networking/gro-lro.html', 'https://www.dpdk.org/', 'https://docs.openvswitch.org/en/latest/intro/ovs-dpdk/']",0,10_programmable_data_planes,0.92,1
119,What are some of the problems with current OpenFlow hardware that motivated a custom OpenFlow chip design? Select all that apply.,"a. Current hardware makes it difficult to implement multiple match action stages, and to repurpose TCAM and SRAM across multiple match-action stages.
b. Current hardware cannot forward traffic at line rates.
c. The current OpenFlow protocol depends on the constraints of conventional switching chips.
d. Current hardware cannot support OpenFlow 1.0 match-action primitives.
e. Current hardware is difficult to repurpose in the field.
",,"a,c,e","a,c,e",,"a: True — conventional switching ASICs have fixed pipeline stages and statically partitioned resources (TCAM/SRAM), making it hard to implement arbitrary multiple match-action stages or to dynamically repurpose memory across stages. c: True — early OpenFlow designs (e.g., v1.0) were defined to match what conventional switching chips could do, so the protocol reflects those hardware constraints. e: True — current commodity switching ASICs are largely fixed-function and vendor-controlled, making it difficult to repurpose or reprogram them in the field. 

Incorrect options: b is false because commodity switching ASICs are designed to forward at line rate for their supported functions; the problem motivating custom chips was programmability and flexibility, not basic forwarding throughput. d is false because OpenFlow 1.0 was intentionally designed around the match-action primitives that existing hardware could support.","['https://www.opennetworking.org/wp-content/uploads/2013/04/openflow-spec-v1.0.0.pdf', 'https://home.cs.princeton.edu/~mckeown/papers/openflow-nets09.pdf', 'https://p4.org/ (background on protocol-independent programmable data planes and motivations for custom programmable switch designs)']",1,10_programmable_data_planes,0.9,1
120,"Which of the following are motivations for SwitchBlade, which supports composition of pre-synthesized hardware data plane modules on an FPGA? Select all that apply.","a. Most data plane protocols involve composing only a limited number of data-plane actions.
b. Experimental data planes may need to operate in parallel with production data planes.
c. Programming in Verilog is difficult.
d. FPGAs are lower-cost and consume less power than conventional ASICs or OpenFlow switching chips.
e. Programming in C is difficult.
",,"a,b,c","a,b,c",,"SwitchBlade was motivated by the desire to compose pre-synthesized, reusable dataplane modules so new protocols can be built by assembling a small set of actions (a). The design also explicitly targeted safe experimentation — allowing experimental dataplanes to run alongside production dataplanes (b). Another stated motivation was to reduce the burden of low-level FPGA development: programming in Verilog/HDLs and the long synthesis/PNR cycle is difficult, so pre‑synthesized composable modules ease development (c). Option d is incorrect because FPGAs are generally more flexible but not lower-cost or lower-power than custom ASICs; option e is not a stated motivation (the difficulty noted is for Verilog/HDLs, not C).","['https://www.researchgate.net/publication/220723052_SwitchBlade_a_Platform_for_Rapid_Deployment_of_Network_Protocols_on_Programmable_Hardware', 'https://www.xilinx.com/support/documentation/white_papers/wp416-fpga-vs-asics.pdf (Xilinx white paper on FPGA vs ASIC tradeoffs)', 'https://en.wikipedia.org/wiki/Hardware_description_language (background on Verilog/HDL complexity)']",1,10_programmable_data_planes,0.88,1
